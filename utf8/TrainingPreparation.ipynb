{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Preparation\n",
    "\n",
    "This document states the goals for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "For the current work the training is focused in learning language representations for:\n",
    "* multi-lingual\n",
    "* multi-task\n",
    "* Text-to-Text (as in T5 from DeepMind) \n",
    "\n",
    "With the goal of:\n",
    "\n",
    "* [Few-Shot|Zero-Shot] learning of new tasks\n",
    "* Zero-Shot translation for language pairs\n",
    "* Being able to add new knowledge without catastrophic forgetting\n",
    "\n",
    "The particularity of the trained model is that they are intended to be able to recognize and solve the given tasks solely from the description in the input. The models should be able to perform well enough without any retraining or fine-tunning. Any re-training or fine-tunning should also be able to be done without hindering (much?) the previously trained tasks.\n",
    "\n",
    "Also there is the important point to make that all this work focuses on having an input as RAW as possible (meaning no text normalization, separation, .... etc) and having NO Out of Vocabulary (OOV) input. So all the languages should be feasible as input.\n",
    "\n",
    "\n",
    "## Model Outputs\n",
    "\n",
    "Particularly the model should have multiple outputs being those the following:\n",
    "\n",
    "1. Origin Language (language detection) - Language CODE (2 or 3 character code)\n",
    "2. Destination Language (the original one if none given) - Language CODE (2 or 3 character code)\n",
    "3. Task Language (The language in which the task was described) - Language CODE (2 or 3 character code)\n",
    "4. The task at hand\n",
    "5. **The TARGET output** of the task for the current input\n",
    "\n",
    "~~5. PoS UPOS tagging ckech if needed or useful?~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The current work follows the example of [ERNIE 2.0](https://arxiv.org/abs/1907.12412) in that uses supervised training to improve the performance. In that paper they show that supervised tasks allow better training than just Denoising LM target with unsupervised learning. \n",
    "In this work we leverage many other tasks and meta-learning to take advantage of existing mono and multi-lingual training datasets.\n",
    "\n",
    "The training will be done in the following stages:\n",
    "\n",
    "1. A general pre-training with many datapoints on all the tasks\n",
    "    - Evaluation on different validation sets\n",
    "2. A few-shot meta-learninig training based on the pre-trained weights\n",
    "    - Evaluation on different validation sets\n",
    "3. Add architectural changes to the network (parallel columns and memory) and train for tasks with the meta-learned \n",
    "    - Evaluation on different validation sets and tasks\n",
    "4. Overfit new input or input that fails -> create an entry in the memory for it.\n",
    "    - Evaluate\n",
    "5. ... ?\n",
    "\n",
    "\n",
    "The intuition between all this is:\n",
    " - Having a strong statistical baseline with many tasks\n",
    " - Meta-learning to be able to quickly learn new tasks\n",
    " - Use the few-shot learn ability to be able to add new knowledge in the memory as in [Large Memory Layers with Product Keys](https://arxiv.org/abs/1907.052420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tasks\n",
    "\n",
    "The tasks to prepare for the training, many of these are intended solely for training and there is no interest in a validation dataset for these for comparison with other models as they are not available in the literature. The current work does not intend to add new tasks or metrics.\n",
    "\n",
    "The default task is to denoise and correct the input (if for example there are multiple spaces together, or diacritics missing, uppercases, etc).\n",
    "\n",
    "Many of the tasks can be taken from standard Text pre-processing tasks, the idea being here to use available weakly-supervised with minimal transformations in the input.\n",
    "\n",
    "The tasks can be separated into:\n",
    "\n",
    "### [Un|Weakly]-supervised\n",
    "\n",
    "* MLM (Masked Language Model) BERT|mBERT|BART|ERNIE training objective (TODO) \n",
    "* Denoising\n",
    "* Capitalization\n",
    "* to-Lowercase\n",
    "* to-UPPERCASE\n",
    "* Add diacritics\n",
    "* Remove Diacritics\n",
    "\n",
    "\n",
    "### Supervised\n",
    "\n",
    "* [GLUE](https://gluebenchmark.com/)\n",
    "* [SuperGLUE](https://super.gluebenchmark.com/); [Paper](https://w4ngatang.github.io/static/papers/superglue.pdf) \n",
    "* Grammar tagging and recognition (PoS, NER, Dependency detection, ...) [UD-Treebank v2.5](https://universaldependencies.org/)\n",
    "* Translation [WikiMatrix](https://ai.facebook.com/blog/wikimatrix/); [Paper](https://arxiv.org/abs/1907.05791); [Github](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO but easy to make during data preparation:\n",
    "\n",
    "* Add accents to string in ASCII ,remove accents (for French, Spanish and other languages like these) .... [Answer in stackoverflow](https://stackoverflow.com/questions/517923/what-is-the-best-way-to-remove-accents-in-a-python-unicode-string) and [open science](https://www.science-emergence.com/Articles/How-to-remove-string-accents-using-python-3/) with [Gensim](https://radimrehurek.com/gensim/utils.html#gensim.utils.deaccent), should also do Remove Diacritics ...\n",
    "* Fill the placeholder (for example eliminate only one word and make it fill it ...?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO - prepare and do the following too:\n",
    "\n",
    "* Tatoeba:  Wikimatrix is nice but this one has different kind of phrases (questions, answers and some other things)\n",
    "* [EuroParliament](http://www.statmt.org/europarl/)\n",
    "* [Wikipedia Translation Dataset](http://opus.nlpl.eu/Wikipedia.php); [WikiExtractor](https://github.com/tatuylonen/wiktextract)\n",
    "* [ConceptNET](http://conceptnet.io/); [Github](https://github.com/commonsense/conceptnet5/wiki) \n",
    "* [Open Multilingual WordNet](http://compling.hss.ntu.edu.sg/omw/) and [Global WordNet Association](http://globalwordnet.org/resources/wordnets-in-the-world/)\n",
    "\n",
    "\n",
    "* Conjugate verbs for different languages -> do as with the language textbooks !!!\n",
    "* Give the dictionary definition of word ...\n",
    "\n",
    "\n",
    "While the starting datasets and tasks are given here, there are many others like multi-hop search and question answering that I would love to add later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Corruption techniques\n",
    "\n",
    "### BART pre-training details - Noise Generation Techniques:\n",
    "\n",
    "* Token Masking\n",
    "* Token Deletion\n",
    "* Tex Infilling\n",
    "* Sentence Permutation\n",
    "* Document Rotation\n",
    "\n",
    "### Add a noise layer every N layers in the NN\n",
    "\n",
    "TODO, this is a technique that should be tested too, this might be enough for another paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks to test\n",
    "\n",
    "The Neural Networks to test will be the following:\n",
    "\n",
    "* Vanilla CNN (autoencoder like, non seq2seq but the input length can change if fully convolutional), this is for easy baseline\n",
    "* ~~Vanilla LSTM ?? -> might be slow/heavy to train~~\n",
    "* Vanilla Transformer Encoder only (non seq2seq)\n",
    "* Vanilla Transformer Decoder only (non seq2seq??)\n",
    "* Transformer (Encoder-Decoder Seq2Seq architecture - limit the number of parameters here, to study)\n",
    "* [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)\n",
    "* Hybrid Seq2Seq (description below - goal to cut as many params as possible and ):\n",
    "    - CNN + Light Dynamic Convolutions at input level  (if possible compress the time dimension)\n",
    "    - Encoder [Transformer|Reformer] layers With Large Memory \n",
    "    - Decoder [Transformer|Reformer] Layers with Large Memory\n",
    "    - Deconv CNN +  Light Dynamic Convolution (to decompress the input time dimension into tokens)\n",
    "\n",
    "\n",
    "### Modifications to network and training\n",
    "\n",
    "All networks should also test the following modifications (though I incline myself to just do it in the hybrid architecture due to expense) \n",
    "\n",
    "* Use REPTILE (better for rsource usage than MAML) for meta-training\n",
    "* Use Plastic (hebbian?) + Modulated Plastic meta-learning\n",
    "* Use NeuralDB\n",
    "* Use Overfitting for new tasks and for examples where there are errors, overfit in a new memory position and later do attention over the memory outputs\n",
    "* The encoder just as a circular convolution of the inputs instead of making some comples NN ?? \n",
    "* Use [The Evolved Transformer](https://arxiv.org/abs/1901.11117) instead of the base Transformer (need to be implemented in pytorch)\n",
    "* Use Relative Positional Encoding and keeping past activations from TransformerXL\n",
    "* Use Compressive Transformers' ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Tasks and Metrics\n",
    "\n",
    "The tasks that will be given importance at the validation moment and should be used for comparison with other SoTA models available in the literature.\n",
    "\n",
    "* GLUE: standard by the site's validation set\n",
    "* SuperGLUE: standard by the site's validation set\n",
    "* Translation [BLEU](https://en.wikipedia.org/wiki/BLEU), [F-Score](https://en.wikipedia.org/wiki/F1_score), [ROUGE](https://en.wikipedia.org/wiki/F1_score) ... ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Description and decisions\n",
    "\n",
    "\n",
    "* As with [BART](https://arxiv.org/pdf/1910.13461.pdf) use a *Bidirectional Encoder* with an *Autoregressive Decoder* \n",
    "\n",
    "* The model will be a Seq2Seq based on the transformer architecture, the input and output sizes should be of dynamic dimension. The input dimension, if possible will be compressed by the network (example, from an original input of 1024 go to a dimension of 384 or 256).\n",
    "* The dimensions will be divisible by 2, and mostly power of 2\n",
    "\n",
    "* The model will have as an input a character level of 128 dimensions based on 3-segment UTF-8 text encoder developed previously by the same author. No OOV characters should exist for most languages and input types (only emojis and extended lost languages as egyptian will not be represented, all that is represented by the 4th segment on the UTF-8 encoding, the last code-point used will be U+FFFF). This is to be able to encode most (if not all) the languages in the available datasets for the current work. Using a 2-segment based coding for utf-8 would be much more memory and processor savy but would leave JCK languages OOV which we don't want.\n",
    "\n",
    "* The decoding from vector to text will be done at character-level based on the [FAISS facebook's library](https://github.com/facebookresearch/faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tools\n",
    "\n",
    "* Development in PyTorch (easiest for research and the one the authors know best)\n",
    "* Tensorboard with torch-tensorboard to check during training\n",
    "* Mixed Precision training with [NVIDIA Apex](https://github.com/NVIDIA/apex)\n",
    "\n",
    "## Training Techniques:\n",
    "\n",
    "Most of the issues in training will be due to memory, so for this there are the following techniques to use:\n",
    "\n",
    "* [Gradient Checkpointing](https://arxiv.org/abs/1604.06174); [Fitting larger networks into memory.\n",
    "](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) in [Pytorch](https://qywu.github.io/2019/05/22/explore-gradient-checkpointing.html)\n",
    "* [Reformer - Google](https://arxiv.org/pdf/2001.04451.pdf)\n",
    "* [Reversible Residual Network: Backpropagation Without Storing Activations](https://arxiv.org/abs/1707.04585) [some](https://github.com/tbung/pytorch-revnet) [sources](https://github.com/renmengye/revnet-public)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Specifications\n",
    "\n",
    "The entire development, pre-processing and training is to be limited to the following software and HW:\n",
    "\n",
    "    System Ubuntu 19.04\n",
    "    Python 3.7\n",
    "    PyTorch+=1.4\n",
    "\n",
    "\n",
    "    CPU = 8 core intel i7700\n",
    "    GPU_1 = RTX 2080 Ti - 11GB RAM\n",
    "    GPU_2 = GTX 1080 - 8 GB RAM ( -1GB for the system that uses it)\n",
    "    RAM = 64 GB\n",
    "    Local Disk =  1TB NVMe\n",
    "    Remote NFS mounted = RAID1 4TB (on Raspberry-PI 4+ 4GB) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding\n",
    "\n",
    "Although feasible with the proposed encoding, [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) compression will not be dealt with in the current and is left as future work. The posibilities for BPE are:\n",
    " * Circular Convolution (in the given order) <- I would bet for this encoding type: in pytorch is with padding_type = circular\n",
    " * Additive (although this one can be a problem)\n",
    " * Other ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
