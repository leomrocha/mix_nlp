{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This notebook develops the data preparation for text-to-text learning for supervised datasets (like T5 from Deep Mind), it extends T5 for more tasks and is developed with PyTorch.\n",
    "\n",
    "The source code is open-sourced.\n",
    "\n",
    "For the processed text, it will be given when/if I get resources to get it in the open.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation.\n",
    "\n",
    "One of the ideas of this process is to do less pre-processing and use the least pre-processed text possible. Uppercase, punctuation and other simbols have information that with some pre-processing is lost. This might not be too problematic for English or other languages, but certainly is for German (and might be for others).\n",
    "\n",
    "Due to this, many of the pre-processsd (tokenized) datasets available are discarded and the data preparation will be done from Raw data (example for the GLUE and SuperGLUE benchmmarks)\n",
    "\n",
    "Data preparation would be much faster with Scala in Spark than with Python but for ease of portability and usage I'll be using python. Also the data preparation is one off only, no need to re-process once done.\n",
    "\n",
    "Nevertheless, even if working with Python, choosing the right libraries is good. This is why for json we choose [orjson](https://github.com/ijl/orjson) and for csv even though there seems to be a [faster library ](https://github.com/juancarlospaco/faster-than-csv) it does not have many users or community so we keep with the standard csv library which is the fastest other way of doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Task Description\n",
    "\n",
    "In the original T5 paper the tasks are described in english and with a single representation, for example: \n",
    " \n",
    "    Source String: \"translate {}\"\n",
    "    Target String: \"to {}\"\n",
    " \n",
    "In this work we add a few variations to this. The first variation is that the task will be described in multiple languages, for starting:\n",
    "\n",
    "* English\n",
    "* Spanish\n",
    "* French\n",
    "* German\n",
    "\n",
    "The second change is that instead of a single description of the task, there will be multiple ones and they'll be chosen randomly.\n",
    "\n",
    "Examples for language translation:\n",
    " \n",
    "    \" Cómo se dice: {} en {} ?\"\n",
    "    \" Cómo se escribe: {} en {} ?\"\n",
    "    \" Escribe: {} en {} ?\"\n",
    "    \" Traducir: {} al {}.\"\n",
    "    \" Por favor traduce: {} al {}\"\n",
    "    \" Traduce: {} al {}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets List to process/analyze\n",
    "\n",
    "* ~~MUSE~~ Issue downloading data, only multilang dictionaries available\n",
    "* GLUE <-\n",
    "    - [CoLA](https://nyu-mll.github.io/CoLA/); [Neural Network Acceptability Judgments ](https://arxiv.org/abs/1805.12471); [Source Code](https://github.com/nyu-mll/CoLA-baselines)\n",
    "    - [MNLI](https://www.nyu.edu/projects/bowman/multinli/); [Paper](https://arxiv.org/abs/1704.05426); [Baseline](https://github.com/nyu-mll/multiNLI/blob/master/README.md)\n",
    "    - MRPC [Paper](https://pdfs.semanticscholar.org/13d7/cbe9035abbb0f243a5e63e19d9c01bcf69d8.pdf); [Original Dataset](https://www.microsoft.com/en-us/download/details.aspx?id=52398&from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F607d14d9-20cd-47e3-85bc-a2f65cd28042%2F)\n",
    "    - QNLI [Paper](https://www.nyu.edu/projects/bowman/glue.pdf) \n",
    "    - QQP\n",
    "    - RTE\n",
    "    - SNLI\n",
    "    - SST-2\n",
    "    - STS-B\n",
    "    - WNLI\n",
    "* SuperGLUE <-\n",
    "    - BoolQ\n",
    "    - CB\n",
    "    - COPA\n",
    "    - MultiRC\n",
    "    - ReCoRD\n",
    "    - RTE\n",
    "    - WiC\n",
    "    - WSC\n",
    "* MultiNLI <- \n",
    "* SNLI\n",
    "* XNLI <- this one is interesting\n",
    "* UD-Treebank v2.5 <- this one is interesting\n",
    "* SWAG\n",
    "* WikiMatrix <- this one is interesting\n",
    "* ~~[SETimes](http://nlp.ffzg.hr/resources/corpora/setimes/)~~ No need of it, already many samples at WikiMatrix and UD-Treebank\n",
    "* Tatoeba:  Wikimatrix is nice but this one has different kind of phrases (questions, answers and some other things)\n",
    "* EuroParliament"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Datasets\n",
    "\n",
    "* Gutenberg\n",
    "* Wiktionary\n",
    "* Wikipedia\n",
    "* ArXiv\n",
    "* Wikitext-2\n",
    "* Wikitext-103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoLA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import cola_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.62 ms, sys: 20.3 ms, total: 30 ms\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "%time cola_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI - MultiNLI Dataset\n",
    "\n",
    "There are more than one task that are possible as the dataset contains also the parse tree for each sentence, which is nice. So the output format of the json will be:\n",
    "\n",
    "    {\n",
    "        'input': \"task: MNLI | Sentence 1: {} | Sentence 2: {}\".format(sentence_1, sentence_2),\n",
    "        'target': e['gold_label'],\n",
    "        'input_sentence_1': \"task: MNLI parse tree of: {}\".format(sentence_1),\n",
    "        'input_sentence_2': \"task: MNLI parse tree of: {}\".format(sentence_2),\n",
    "        'parse_target_1': e['sentence1_parse'],\n",
    "        'parse_target_2': e['sentence2_parse'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import mnli_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 ms, sys: 27.8 ms, total: 29.8 ms\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%time mnli_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRPC \n",
    "\n",
    "\n",
    "\n",
    "This data consists of 5 columns:\n",
    "\n",
    "    label: 0 Not equivalent, 1 semantically equivalent\n",
    "    sentence 1 id\n",
    "    sentence 2 id\n",
    "    sentence 1 text\n",
    "    sentence 2 text\n",
    "    \n",
    "    \n",
    "    \n",
    "The note to make is that the dataset is already tokenized meaning is not the raw text. Nothing else will be done to the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNLI\n",
    "\n",
    "The dataset download contains the following columns:\n",
    "\n",
    "    ndex\n",
    "    Question\n",
    "    Sentence\n",
    "    Label - [entailment|not_entailment]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQP\n",
    "\n",
    "Columns in the dataset:\n",
    "\n",
    "    id\n",
    "    qid1\n",
    "    qid2\n",
    "    question1\n",
    "    question2\n",
    "    is_duplicate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
