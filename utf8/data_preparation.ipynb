{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This notebook develops the data preparation for text-to-text learning for supervised datasets (like T5 from Deep Mind), it extends T5 for more tasks and is developed with PyTorch.\n",
    "\n",
    "The source code is open-sourced.\n",
    "\n",
    "For the processed text, it will be given when/if I get resources to get it in the open.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation.\n",
    "\n",
    "One of the ideas of this process is to do less pre-processing and use the least pre-processed text possible. Uppercase, punctuation and other simbols have information that with some pre-processing is lost. This might not be too problematic for English or other languages, but certainly is for German (and might be for others).\n",
    "\n",
    "Due to this, many of the pre-processsd (tokenized) datasets available are discarded and the data preparation will be done from Raw data (example for the GLUE and SuperGLUE benchmmarks)\n",
    "\n",
    "Data preparation would be much faster with Scala in Spark than with Python but for ease of portability and usage I'll be using python. Also the data preparation is one off only, no need to re-process once done.\n",
    "\n",
    "Nevertheless, even if working with Python, choosing the right libraries is good. This is why for json we choose [orjson](https://github.com/ijl/orjson) and for csv even though there seems to be a [faster library ](https://github.com/juancarlospaco/faster-than-csv) it does not have many users or community so we keep with the standard csv library which is the fastest other way of doing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Task Description\n",
    "\n",
    "In the original T5 paper the tasks are described in english and with a single representation, for example: \n",
    " \n",
    "    Source String: \"translate {}\"\n",
    "    Target String: \"to {}\"\n",
    " \n",
    "In this work we add a few variations to this. The first variation is that the task will be described in multiple languages, for starting:\n",
    "\n",
    "* English\n",
    "* Spanish\n",
    "* French\n",
    "* German\n",
    "\n",
    "The second change is that instead of a single description of the task, there will be multiple ones and they'll be chosen randomly.\n",
    "\n",
    "Examples for language translation:\n",
    " \n",
    "    \" Cómo se dice: {} en {} ?\"\n",
    "    \" Cómo se escribe: {} en {} ?\"\n",
    "    \" Escribe: {} en {} ?\"\n",
    "    \" Traducir: {} al {}.\"\n",
    "    \" Por favor traduce: {} al {}\"\n",
    "    \" Traduce: {} al {}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets List to process/analyze\n",
    "\n",
    "* ~~MUSE~~ Issue downloading data, only multilang dictionaries available\n",
    "* GLUE\n",
    "    - [CoLA](https://nyu-mll.github.io/CoLA/); [Neural Network Acceptability Judgments ](https://arxiv.org/abs/1805.12471); [Source Code](https://github.com/nyu-mll/CoLA-baselines)\n",
    "    - [MNLI](https://www.nyu.edu/projects/bowman/multinli/); [Paper](https://arxiv.org/abs/1704.05426); [Baseline](https://github.com/nyu-mll/multiNLI/blob/master/README.md)\n",
    "    - MRPC [Paper](https://pdfs.semanticscholar.org/13d7/cbe9035abbb0f243a5e63e19d9c01bcf69d8.pdf); [Original Dataset](https://www.microsoft.com/en-us/download/details.aspx?id=52398&from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdownloads%2F607d14d9-20cd-47e3-85bc-a2f65cd28042%2F)\n",
    "    - QNLI [Paper](https://www.nyu.edu/projects/bowman/glue.pdf) \n",
    "    - QQP\n",
    "    - RTE\n",
    "    - SNLI\n",
    "    - SST-2\n",
    "    - STS-B\n",
    "    - WNLI\n",
    "* [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf) \n",
    "    - BoolQ\n",
    "    - CB\n",
    "    - COPA\n",
    "    - MultiRC\n",
    "    - ReCoRD\n",
    "    - RTE\n",
    "    - WiC\n",
    "    - WSC\n",
    "* XNLI <- this one is interesting\n",
    "* UD-Treebank v2.5 <- this one is interesting\n",
    "* [SWAG](http://rowanzellers.com/swag/); [Paper](https://arxiv.org/abs/1808.05326); [Source Code](https://github.com/rowanz/swagaf)\n",
    "* [WikiMatrix](https://ai.facebook.com/blog/wikimatrix/); [Paper](https://arxiv.org/abs/1907.05791); [Github](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix)\n",
    "* ~~[SETimes](http://nlp.ffzg.hr/resources/corpora/setimes/)~~ No need of it, already many samples at WikiMatrix and UD-Treebank\n",
    "* Tatoeba:  Wikimatrix is nice but this one has different kind of phrases (questions, answers and some other things)\n",
    "* [EuroParliament](http://www.statmt.org/europarl/)\n",
    "* [Wikipedia Translation Dataset](http://opus.nlpl.eu/Wikipedia.php); [WikiExtractor](https://github.com/tatuylonen/wiktextract)\n",
    "* [ConceptNET](http://conceptnet.io/); [Github](https://github.com/commonsense/conceptnet5/wiki) \n",
    "* [Open Multilingual WordNet](http://compling.hss.ntu.edu.sg/omw/) and [Global WordNet Association](http://globalwordnet.org/resources/wordnets-in-the-world/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Datasets\n",
    "\n",
    "* Gutenberg\n",
    "* [Wiktionary](https://dumps.wikimedia.org/enwiktionary/)\n",
    "* Scholarpedia\n",
    "* [Wikipedia](https://dumps.wikimedia.org/)\n",
    "* ArXiv\n",
    "* Wikitext-2\n",
    "* Wikitext-103 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoLA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI - MultiNLI Dataset\n",
    "\n",
    "There are more than one task that are possible as the dataset contains also the parse tree for each sentence, which is nice. So the output format of the json will be:\n",
    "\n",
    "    {\n",
    "        'input': \"task: MNLI | Sentence 1: {} | Sentence 2: {}\".format(sentence_1, sentence_2),\n",
    "        'target': e['gold_label'],\n",
    "        'input_sentence_1': \"task: MNLI parse tree of: {}\".format(sentence_1),\n",
    "        'input_sentence_2': \"task: MNLI parse tree of: {}\".format(sentence_2),\n",
    "        'parse_target_1': e['sentence1_parse'],\n",
    "        'parse_target_2': e['sentence2_parse'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRPC \n",
    "\n",
    "\n",
    "\n",
    "This data consists of 5 columns:\n",
    "\n",
    "    label: 0 Not equivalent, 1 semantically equivalent\n",
    "    sentence 1 id\n",
    "    sentence 2 id\n",
    "    sentence 1 text\n",
    "    sentence 2 text\n",
    "    \n",
    "    \n",
    "    \n",
    "The note to make is that the dataset is already tokenized meaning is not the raw text. Nothing else will be done to the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QNLI\n",
    "\n",
    "The dataset download contains the following columns:\n",
    "\n",
    "    ndex\n",
    "    Question\n",
    "    Sentence\n",
    "    Label - [entailment|not_entailment]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QQP\n",
    "\n",
    "Columns in the dataset:\n",
    "\n",
    "    id\n",
    "    qid1\n",
    "    qid2\n",
    "    question1\n",
    "    question2\n",
    "    is_duplicate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import process_glue, process_superglue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening /home/leo/projects/Datasets/text/GLUE/MNLI/original/multinli_1.0_dev_mismatched.jsonl\n",
      "opening /home/leo/projects/Datasets/text/GLUE/CoLA/dev.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/CoLA/test.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/CoLA/train.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/MRPC/dev_ids.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/MNLI/original/multinli_1.0_dev_matched.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/CoLA/dev-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MRPC/dev_ids-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/CoLA/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/MRPC/test.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/MNLI/original/multinli_1.0_train.jsonl\n",
      "opening /home/leo/projects/Datasets/text/GLUE/MRPC/dev.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/MRPC/train.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MRPC/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/QNLI/test.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MRPC/dev-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/QNLI/dev.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/QNLI/train.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MRPC/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/CoLA/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/QNLI/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/QQP/train.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/QQP/dev.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/QNLI/dev-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/QQP/test.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/RTE/train.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/RTE/train-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/RTE/test.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/RTE/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/RTE/dev.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MNLI/original/multinli_1.0_dev_matched-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MNLI/original/multinli_1.0_dev_mismatched-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/RTE/dev-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/SNLI/original/snli_1.0_dev.jsonl\n",
      "opening /home/leo/projects/Datasets/text/GLUE/SNLI/original/snli_1.0_train.jsonl\n",
      "opening /home/leo/projects/Datasets/text/GLUE/SNLI/original/snli_1.0_test.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/QQP/dev-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/SNLI/original/snli_1.0_test-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/SNLI/original/snli_1.0_dev-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/SST-2/train.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/SST-2/test.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/SST-2/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/SST-2/dev.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/STS-B/train.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/SST-2/dev-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/STS-B/test.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/STS-B/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/STS-B/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/STS-B/dev.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/WNLI/train.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/STS-B/dev-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/WNLI/train-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/GLUE/WNLI/test.tsv\n",
      "opening /home/leo/projects/Datasets/text/GLUE/WNLI/dev.tsv\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/WNLI/dev-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/WNLI/test-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/SST-2/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/QNLI/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/QQP/test-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/QQP/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/MNLI/original/multinli_1.0_train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/GLUE/SNLI/original/snli_1.0_train-txt2txt.json\n",
      "CPU times: user 42.1 ms, sys: 25.5 ms, total: 67.6 ms\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%time process_glue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperGLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening /home/leo/projects/Datasets/text/SuperGLUE/BoolQ/test.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/CB/val.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/BoolQ/val.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/CB/test.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/BoolQ/train.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/ReCoRD/val.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/CB/val-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/CB/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/RTE/val.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/ReCoRD/train.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/CB/train.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/ReCoRD/test.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/CB/train-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/RTE/test.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/RTE/val-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/RTE/test-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/BoolQ/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/WiC/val.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/WiC/test.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/BoolQ/val-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/RTE/train.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/WiC/test-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/WiC/val-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/WiC/train.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/WSC/val.jsonl\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/WSC/test.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/BoolQ/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/WSC/test-txt2txt.json\n",
      "opening /home/leo/projects/Datasets/text/SuperGLUE/WSC/train.jsonl\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/WSC/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/RTE/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/WSC/val-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/WiC/train-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/ReCoRD/val-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/ReCoRD/test-txt2txt.json\n",
      "saving to /home/leo/projects/Datasets/text/SuperGLUE/ReCoRD/train-txt2txt.json\n",
      "CPU times: user 23.4 ms, sys: 14.4 ms, total: 37.8 ms\n",
      "Wall time: 843 ms\n"
     ]
    }
   ],
   "source": [
    "%time process_superglue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwagAF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Dependencies v2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from preprocess_conllu import conllu_process\n",
    "from preprocess_conllu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 ms, sys: 46.1 ms, total: 86.3 ms\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "conllu_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wm = get_all_files_recurse(\"/media/nfs/Datasets/text/WikiMatrix/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiMatrix\n",
    "\n",
    "File structure is:\n",
    " \n",
    "    v1/*.gz - 65 GB\n",
    "    vi/SMALL/*.gz - 4,6GB\n",
    "    \n",
    "We can use all the big files for the training and the small ones for validation. Checking the files they are different language pairs, so this can be used for Zero-Shot learning on translation pairs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess_wikimatrix import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wikimedia_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3545"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1620, 1925])  # 1620 are the complete files, 1925 are the files in the SMALL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKIMATRIX_BASEPATH = \"/media/nfs/Datasets/text/WikiMatrix/v1\"\n",
    "\n",
    "allfiles = get_all_files_recurse(WIKIMATRIX_BASEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t = [f for f in allfiles if 'txt2txt' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t2t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3545 files processed and 3545 files existing, everything seems OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
