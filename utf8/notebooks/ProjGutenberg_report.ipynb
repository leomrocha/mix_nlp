{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical Analysis of more than 50000 Books from Project Gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "With the advent of deep neural networks, growing computing power and improvements in techniques (as Transformers networks) there is a growing interest in Natural Language Processing (NLP) and Natural Language Understanding (NLU) on the scientific comunity, this has also led to more practical use cases in industry and libraries such as [Rasa](TODO) and [SpaCy](TODO) have arised.\n",
    "\n",
    "In the scientific comunity the most recent tendency is to construct big transformer networks with billions of parameters and put them through enormous unsupervised learning datasets. As an example the latest [GPT-3 model](https://arxiv.org/abs/2005.14165) contains 175 Billion parameters, a single trainig run has been estimated to cost [4.6M USD](https://lambdalabs.com/blog/demystifying-gpt-3/#:~:text=%244%2C600%2C000%3A%20The%20full%20cost%20of,per%2Dhour%20GPUs%20on%20market.) on commercially available hardware. \n",
    "\n",
    "From the practical point of view, these networks are impossible to train and use for small and medium sized companies in a commercial setup due to several problems, including training and operation costs as well as latency and throughput of these networks. This is the reason software as SpaCy use a more manual approach having lookup tables to the first stages of text processing. Nevertheless there is always in today's setup a deep neural network layer, be a transformer, CNN, or a variation of RNN, being the most popular of the Recurrent NNs the LSTM architecture. Transformers are getting more market share as they are more powerful in practice (even if both are \"Turing Complete\") and easier to train and use than LSTMs.\n",
    "\n",
    "From the design point of view, a network must be designed to fit the use cases, which in part means to fit the inputs, for this purpose statistics and knowledge about the domain are necessary. The information provided in this study is intended to help in that decision.\n",
    "\n",
    "From our knowledge, even if there are some [simple statistics available](http://www.gutenbergnews.org/statistics/) this is the first work that presents a thorough statistical analysis on (most of) the books present in [Project Gutenberg](https://www.gutenberg.org/) at the date of 30st May 2020. \n",
    "\n",
    "This work is intended as a help for lingüists and scientist working with language data.\n",
    "\n",
    "Note that this is a continuation of the previous work [A Statistical Exploration of Universal Dependencies Conllu Files](https://leomrocha.github.io/ud_conllu_v2.6/index.html) by the [same author](https://leomrocha.github.io/) on statistic analysis of NLP and lingüistic datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Results Files\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org/) books up to 30th May 2020. \n",
    "[Project Gutenberg Catalog](https://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs) to be able to get metadata on each book instead of having to find it and parse it from the text.\n",
    "\n",
    "\n",
    "The original data:\n",
    "* 16 GB compressed (zipped)\n",
    "* 91793 (zipped) books, many books appear two or more times in more than one format\n",
    "* 59897 (zipped) unique books\n",
    "* RDF Database Catalog 1.3 GB\n",
    "\n",
    "There are many books that are not presented in the results, this is due to errors during processing or book size which limited the capacity on my PC.\n",
    "\n",
    "The results files are:\n",
    "* ID.stats.json.gz -> global statistics about the file ID\n",
    "* ID.stats_all.json.gz -> complete statistics including count for every paragraph, sentence and token length and word usage\n",
    "* Total of 8.0 GB of compressed data.\n",
    "\n",
    "Note: If new data is added the process is the same for any new book, but the global per language statistics will need to be recomputed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Stack\n",
    "\n",
    "The analysis was done with the following software stack:\n",
    "* Ubuntu 20.04\n",
    "* python 3.8.5 with all dependencies installed in a virtualenv \n",
    "* [pycountry  v19.8.18](https://pypi.org/project/pycountry/) is used to transform between country code and country names\n",
    "* [NumPy v1.18.4](https://numpy.org/) is used for numerical computation\n",
    "* [Pandas v1.0.4](https://pandas.pydata.org/) is used for DataFrame generation and making easier some operations\n",
    "* [SciPy v1.4.1](https://www.scipy.org/) is used for statistics\n",
    "* [Matplotlib v3.2.1](https://matplotlib.org/) was used for the first graphing iterations during the exploration stage\n",
    "* [Bokeh v2.0.2](https://docs.bokeh.org/en/latest/index.html) used for the latest graph and table generation to make interactive presentations\n",
    "* [Jupyter Lab v2.1.2](https://jupyterlab.readthedocs.io/en/stable/) was used during the entire process from starting the exploration to writing the current report\n",
    "* [spacy v2.3.0](https://spacy.io/usage/v2-3)\n",
    "* Many filtering, counting and sorting operations on files were done with simple bash scripts. They are simpler and faster than any other method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "The analysis takes place in different stages:\n",
    "\n",
    "1. The first stage is to process each file separately computing the statistics and getting the word count and frequency. The result of each is stored separately with the Gutenberg Book ID as name. Two files are written, one with the short version and one with all the results.\n",
    "2. The second stage is to aggregate the results by book language.\n",
    "3. The third stage computes the graphs by book and language, each result is stored \n",
    "\n",
    "For the moment there is no separation by date range, even though language styles change during time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline description for file processing:\n",
    "\n",
    "1. get next file\n",
    "2. get file ID and base name of the file\n",
    "3. unzip it in memory and get ENCODING (if available) from the file (maybe will have to reload the file ... for the moment don't care and I'll treat everything as utf-8 compatible as the encodings I found sampling some languages including tagalog, chinese, japanese seem to be utf-8 compatible)\n",
    "4. Get author (name, alias, dates ... ), title and language, publishing date and editor (for the data available)\n",
    "5. clean gutenberg text\n",
    "6. count total length of the text\n",
    "7. Separate paragraphs I'll do it by at least a `\\n\\n` sequence (might not work for every language .. (I do care but I cant, so somebody that knows the language should correct those) \n",
    "8. count the number of paragraphs ... ?\n",
    "9. separate sentences (again, language depending, I'll do it ~by `.` characters and~ with spacy models ...\n",
    "10. count the number of sentences per paragraph\n",
    "11. separate words -> maybe here would need lemmatization to see some things correctly, but will have issues in non supported languages.\n",
    "12. count number of ~~words~~ tokens per sentence -> tokens is already available in spacy, while words is a bit more .. difficult to define and be sure it works in a coherent way.\n",
    "13. sum number of ~~words~~ tokens per paragraph\n",
    "13. separate chars\n",
    "14. count chars per word, sentence and paragraph\n",
    "15. aggregate all words and count the number of occurrences\n",
    "16. aggregate all the characters and count the number of occurrences\n",
    "17. aggregate and sort results then save a zipped (.gz) file with it\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "There are many issues with the current data analysis, but for the global analysis the numbers should be OK enough.\n",
    "These issues are caused by noise in the data, for example captions and references to images that are not in the current text (and can not be taken into account here)\n",
    "\n",
    "Different writing styles are present.\n",
    "\n",
    "The current analysis' goal is to understand some basic statistics on text from different Languages and Authors, starting to give a more thourough insight on some variations that come into play when deciding implementation specificities for NLP pipelines. \n",
    "\n",
    "The contributions of the current study are:\n",
    "- An Open Source library to analyze the entire Gutenberg project statistics.\n",
    "- A database output containing many of the elements needed for the current analysis, which in future works can be reused as a base for improved insights.\n",
    "- A statistical analysis per book and per language of the Gutenberg project database.\n",
    "\n",
    "As an extra element the code separation is done in a way to allow for some code reusability in other text datasets.\n",
    "\n",
    "There are many languages that are not supported by SpaCy, and so the tokenization might not be be correct.\n",
    "\n",
    "Languages that do not separate paragraphs in the sense of western languages will have wrong data on those statistics as the parser separates them by double line break.\n",
    "\n",
    "SpaCy tokenization is not perfect and makes many mistakes. Most tokens with frequency one are tokenization errors and could be used as use-cases for improving the existing tokenizers in SpaCy. Note that SpaCy is now in **version 3.x** which might have fixed many of the issues present in the current work. Due to lack of time I won't be running the complete analysis again.\n",
    "\n",
    "SpaCy has different models, but there are not for every language, so for the moment I'll try a specific model for the available ones, but the general model for the ones that are not available.\n",
    "\n",
    "For this is that I need to first install all available models from SpaCy, if a medium (md) model is available, this one is preferred instead of the small one (sm), the large ones (lg) are going to be ignored as they are too big for the current processing needs (the needs are quite basic in the current study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
