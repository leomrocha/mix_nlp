{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Experiments\n",
    "\n",
    "To be able to visualize aggregated data by author or language we need to sum up the counts in the distributions. As data is quite large it would be good to find an algorithm that would do it faster. This notebook consists of experiments to find this algorithm and create some test data. The final implemention itself is in `utf8/postprocessors/gutenberg_aggregate.py`.\n",
    "\n",
    "Some ideas on how to do it are from [a geeksforgeeks article](https://www.geeksforgeeks.org/python-sum-list-of-dictionaries-with-same-key/). Also there is a method with Pandas Series summing.\n",
    "\n",
    "We are ignoring `stats['doc']` data in this experiment as it has different nesting level and doesn't have many data.\n",
    "\n",
    "The algorithm should:\n",
    "1. get list of all_stats files from a folder and its subfolders\n",
    "2. summ numbers from each file\n",
    "3. return a dict with the same structure as the source files but with summed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import json   \n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['35902-8.stats_all.json.gz', '39340-8.stats.json.gz', '27711-8.stats.json.gz']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOURCE_DIRECTORY = '../../gutenberg_stats_examples/'\n",
    "# os.listdir(SOURCE_DIRECTORY)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Panzini-Alfredo', 'Praga-Emilio', 'Levi-David']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Italian data for testing as it's larger than example dir, but not so huge as English\n",
    "SOURCE_DIRECTORY = '../../gutenberg_stats/clean_results/it/'\n",
    "os.listdir(SOURCE_DIRECTORY)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18 s ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "def collections_counter(data_source_path):\n",
    "    \n",
    "    aggregated_stats = {\n",
    "        'by_paragraph': {\n",
    "            'sentence_length': Counter(), \n",
    "            'word_length': Counter(), \n",
    "            'token_length': Counter(), \n",
    "            'char_length': Counter(),\n",
    "        }, \n",
    "        'by_sentence': { \n",
    "            'word_length': Counter(), \n",
    "            'token_length': Counter(), \n",
    "            'char_length': Counter(),\n",
    "        }, \n",
    "        'by_word': {\n",
    "            'words': Counter(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # get all files (including sub directories)\n",
    "    paths = list(Path(data_source_path).rglob(\"*stats_all*.json.gz\"))\n",
    "    levels = list(aggregated_stats.keys())\n",
    "    # sum up the stats\n",
    "    for path in paths:\n",
    "        with gzip.open(path) as f:\n",
    "            book = json.loads(f.read())\n",
    "        \n",
    "        stats = book['stats_data']\n",
    "        \n",
    "        for level in levels:\n",
    "            for dist_name, agg_dist in aggregated_stats[level].items():\n",
    "                book_dist = stats[level][dist_name]\n",
    "                \n",
    "                agg_dist.update(book_dist)\n",
    "                \n",
    "\n",
    "    return aggregated_stats\n",
    "\n",
    "\n",
    "collections_counter(SOURCE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for-each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.14 s ± 152 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "def python_for_each_and_sets(data_source_path):\n",
    "    \n",
    "    aggregated_stats = {\n",
    "        'by_paragraph': {\n",
    "            'sentence_length': {}, \n",
    "            'word_length': {}, \n",
    "            'token_length': {}, \n",
    "            'char_length': {},\n",
    "        }, \n",
    "        'by_sentence': { \n",
    "            'word_length': {}, \n",
    "            'token_length': {}, \n",
    "            'char_length': {},\n",
    "        }, \n",
    "        'by_word': {\n",
    "            'words': {},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # get all files (including sub directories)\n",
    "    paths = list(Path(data_source_path).rglob(\"*stats_all*.json.gz\"))\n",
    "    levels = list(aggregated_stats.keys())\n",
    "    # sum up the stats\n",
    "    for path in paths:\n",
    "        with gzip.open(path) as f:\n",
    "            book = json.loads(f.read())\n",
    "        \n",
    "        stats = book['stats_data']\n",
    "        \n",
    "        for level in levels:\n",
    "            for dist_name, agg_dist in aggregated_stats[level].items():\n",
    "                book_dist = stats[level][dist_name]\n",
    "                \n",
    "                keys_union = set(agg_dist.keys()) | set(book_dist.keys())\n",
    "                \n",
    "                for key in keys_union:\n",
    "                    agg_dist[key] = agg_dist.get(key, 0) + book_dist.get(key, 0)\n",
    "                \n",
    "\n",
    "    return aggregated_stats\n",
    "\n",
    "\n",
    "python_for_each_and_sets(SOURCE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.4 s ± 142 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "def pandas_series_add(data_source_path):\n",
    "    aggregated_stats = {\n",
    "        'by_paragraph': {\n",
    "            'sentence_length': pd.Series(dtype=int), \n",
    "            'word_length': pd.Series(dtype=int), \n",
    "            'token_length': pd.Series(dtype=int), \n",
    "            'char_length': pd.Series(dtype=int),\n",
    "        }, \n",
    "        'by_sentence': { \n",
    "            'word_length': pd.Series(dtype=int), \n",
    "            'token_length': pd.Series(dtype=int), \n",
    "            'char_length': pd.Series(dtype=int),\n",
    "        }, \n",
    "        'by_word': {\n",
    "            'words': pd.Series(dtype=int),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # get all files (including sub directories)\n",
    "    paths = list(Path(data_source_path).rglob(\"*stats_all*.json.gz\"))\n",
    "\n",
    "\n",
    "    levels = list(aggregated_stats.keys())\n",
    "    # sum up the stats\n",
    "    for path in paths:\n",
    "        with gzip.open(path) as f:\n",
    "            book = json.loads(f.read())\n",
    "        \n",
    "        stats = book['stats_data']\n",
    "\n",
    "        for level in levels:\n",
    "            for key in aggregated_stats[level].keys():\n",
    "                aggregated_stats[level][key] = aggregated_stats[level][key].add(\n",
    "                    pd.Series(stats[level][key]), fill_value=0\n",
    "                ).astype(int)\n",
    "\n",
    "    # remove pandas\n",
    "    for level in levels:\n",
    "        for key, value in aggregated_stats[level].items():\n",
    "            aggregated_stats[level][key] = value.to_dict()\n",
    "\n",
    "    return aggregated_stats\n",
    "\n",
    "\n",
    "pandas_series_add(SOURCE_DIRECTORY)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Data\n",
    "\n",
    "A simple example with the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is stored as json, lots of nesting\n",
    "books = []\n",
    "\n",
    "for file in os.listdir('../postprocessors/test_data/'):\n",
    "    with gzip.open(f'../postprocessors/test_data/{file}') as f:\n",
    "        book = json.loads(f.read())\n",
    "        books.append(book)\n",
    "    \n",
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc', 'by_paragraph', 'by_sentence', 'by_word']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(book['stats_data'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraph_count': 1060,\n",
       " 'sentence_count': 3636,\n",
       " 'token_count': 23290,\n",
       " 'word_count': 23290,\n",
       " 'char_count': 293612}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book['stats_data']['doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_length', 'token_length', 'char_length']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_book['stats_data']['by_sentence'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 1, '2': 2, '3': 3}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_book['stats_data']['by_sentence']['char_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book = {\n",
    "    'metadata': {}, \n",
    "    'file_stats': {}, \n",
    "    'stats_data': {\n",
    "        'doc': {\n",
    "            'paragraph_count': 1,\n",
    "            'sentence_count': 1,\n",
    "            'token_count': 1,\n",
    "            'word_count': 1,\n",
    "            'char_count': 1\n",
    "        }, \n",
    "        'by_paragraph': {\n",
    "            'sentence_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            }, \n",
    "            'word_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            }, \n",
    "            'token_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            }, \n",
    "            'char_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            },\n",
    "        }, \n",
    "        'by_sentence': { \n",
    "            'word_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            }, \n",
    "            'token_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            }, \n",
    "            'char_length': {\n",
    "                '1': 1,\n",
    "                '2': 2,\n",
    "                '3': 3\n",
    "            },\n",
    "        }, \n",
    "        'by_word': {\n",
    "            'words': {\n",
    "                'my': 1,\n",
    "                'test': 2,\n",
    "                'words': 3\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    'tokens': {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data\n",
    "\n",
    "test_file_names = ['test_book_1_stats_all.json.gz', 'test_book_2_stats_all.json.gz', 'test_book_3_stats_all.json.gz']\n",
    "book = json.dumps(test_book)\n",
    "\n",
    "for file in test_file_names:\n",
    "    with gzip.open(f'../postprocessors/test_data/{file}', 'wt', encoding='utf8') as f:\n",
    "        f.write(book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
