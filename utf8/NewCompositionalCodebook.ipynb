{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Compositional Codebook Preparation\n",
    "\n",
    "For this codebook I'll start from the previous idea where each codepoint was independent and only had a number id.\n",
    "\n",
    "For the new code the idea  is a bit more elaborated where:\n",
    "\n",
    "- each first iteration codepoint depends on the index\n",
    "- from this first iteration new codes are derived in the following way:\n",
    " * for the single char codes:\n",
    "   - normalize char with NFKD (so it is decomposed)\n",
    "   - check if is num, is uppercase, is_special, has_diacritic (and which)\n",
    "   - char to lowercase, char to ascii (or simplest representation)\n",
    "   - code is composed of the concatenation of (to_ascii, lowercase_code, is uppercase|lowercase, is numeric|not numeric, is special|not special, diacritic\n",
    " * for the multiple character codes:\n",
    "   - normalize sequence with NFKD\n",
    "   - encode each character\n",
    "   - conv(to_ascii ..) cat sum (to_ascii) cat conv (to lower) cat sum(to_lower) cat conv (diacritics) cat sum(diacritics) cat charcount cat hasnum, cat isnum ... (TODO, finish deciding which kind of code and what does it contains)\n",
    "\n",
    "\n",
    "The idea is:\n",
    "\n",
    "Each character representation contains more information than a simple index, this should make the network's learning easier and give a way of conversion between upper/lower with and without diacritics.\n",
    "\n",
    "The composed code gives information about the presence or absense of a character (the sums) and the order (the convolutions), this should give subspaces where is easier for similarity and proximity analysis.\n",
    "\n",
    "The issue here is that maybe each subspace part should be considered/processed in parallel while getting some information from the other subspace instead of doing it in a big neural network .... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current assumptions are the following:\n",
    "\n",
    "- Origin language is given by name not detected\n",
    "- Destination language is Given by name, not detected\n",
    "- For training a destination vector will be either checked with similarity search (FAISS) or as a one-hot encoding depending on the resource ussage\n",
    "- The input embeddings mapping will be pre-computed (as in the previous iteration) but the number of input elements will be bigger\n",
    "- The tokenization will be greedy, meaning it will try to span the longest sequences first\n",
    "- unknown input tokens should be tested with the following two encoding protocols:\n",
    "  * only span the longest tokens possible\n",
    "  * encode the entire symbol as per the compositional encoding protocol and let the network treat it as an unknown but tag it as something semantically and gramatically\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial code  would be nice to have a redundant code that manages to make close elements close in subspace and alsosomething to pull them appart enough such as the sum of the subspaces is clear enough in the compositional encoded values.\n",
    "\n",
    "Something like the multihot code for the distantiation and single-cycle-code for the proximity part.\n",
    "\n",
    "Now let's compute the number of codepoints for the base generator code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Steps\n",
    "\n",
    "1. Base Generator Code -> index based of a redundant single-cycle-code + multihot-prime-code\n",
    "2. Single Char Basic Code \n",
    "  - after NFKD normalization\n",
    "  - includes if is uppercase/lowercase, \n",
    "  - if contains a diacritic/accidental,  (check if is better to tell which or just a binary element with this)\n",
    "  - if is a composed symbol (more than one char on the NFKD normalization)\n",
    "  - if is a numeric element\n",
    "  - it contains the basic code for the letter (closest ascii for example ... TODO clarify this)\n",
    "3. Composed Code:\n",
    "  - circ conv of Single Char Codes (dim*2)\n",
    "  - sum of previous codes (dim)\n",
    "  - circ conv of ascii representations (dim*2)\n",
    "  - sum of ascii representations (dim)\n",
    "  - is numeric| is alphanumeric | is all text  (dim=3)\n",
    "  - has diacritic/accidental (a position for each, with the vector size being the max length of the token ... for example 5 or 10, or count the number of accidentals instead) (dim=2)\n",
    "  - is all caps (instead of having each  (dim=2)\n",
    "  - starts with upper (dim=2)\n",
    "  \n",
    "This schema is not the simplest one, and takes work to put it in place, but might (and is what I hope) reduce the number of parameters and training time\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the selection of the desired vector size for the embedding codes, I choose to work on the following ranges:\n",
    "single char code might be 48, composed codes should be of dimension no more than 192 but preferred would be 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the following code:\n",
    "\n",
    "We need to represent at least 1619 characters for one of the selected character codes (I'm trying to cut the number of dimensions for the current resources while keeping a maximum of flexibility, more work on this can give more benefits but I won't spend TOO MUCH more time on this)\n",
    "\n",
    "let's say we use the following code:\n",
    "\n",
    "    multihot-code (3,5,11,13) -> max 2145 codepoints\n",
    "    single-cycle-code (4,6,10,12) -> max 2880 codepoints\n",
    "    is upper|is_lower (dim=2)\n",
    "    contains_diacritic (dim=2)\n",
    "    composed_symbol (dim=2)\n",
    "    is_numeric|is_text|is_symbol (dim=3)\n",
    "    ascii_converted_codepoint (transliteration + normalization + taking diacritics out) -> to reduce to maximum the lang \n",
    "    \n",
    "    total_dimension = 3+5+11+13 + 4+6+10+12 + 2 + 2 + 2 + 3 +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from constants import RESERVED_CODE_SPACE\n",
    "from sparse_encoders import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process \n",
    "\n",
    "Making the process streamline and test it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars len =  1564\n",
      "1 first_symbols len =  0\n",
      "1 all_chars len =  3128\n",
      "2 all_chars len =  1519\n",
      "2 first_symbols len =  3128\n",
      "3 first_symbols len =  842\n",
      "all_base_chars len =  870\n",
      "all_base_chars len =  870\n",
      "charcodes len =  1519\n"
     ]
    }
   ],
   "source": [
    "charcodes = compositional_code_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': '\\n',\n",
       " 'complete_conv': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float16),\n",
       " 'non_accent_conv': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float16),\n",
       " 'complete_sum': array([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float16),\n",
       " 'non_accent_sum': array([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float16),\n",
       " 'casing': [False, False, False, False],\n",
       " 'alnum': [False, False, False],\n",
       " 'len': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charcodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(charcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"codes/compositional_charcode_1519_raw_dicts.pkl\"\n",
    "\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(charcodes, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 15M\n",
      "-rw-r--r-- 1 leo leo 205K févr. 12 22:30  adhoc-codebook-1871.pkl\n",
      "-rw-r--r-- 1 leo leo  20K févr.  7 12:12  all_chars_byline.chars\n",
      "-rw-r--r-- 1 leo leo  15K févr.  7 12:12  all_chars.chars\n",
      "-rw-r--r-- 1 leo leo 1,4M avril 11 14:43  compositional_charcode_1564_raw_dicts\n",
      "-rw-r--r-- 1 leo leo 1,3M avril 11 15:56  compositional_charcode_1564_raw_dicts.pkl\n",
      "-rw-r--r-- 1 leo leo 176K févr. 12 22:30 'utf8_2-seg_1871-codepoints_96-dim_N-24-k3_coprimes-(3, 5, 11, 13)_cycles-(4, 6, 8, 10, 12)_dense.npy'\n",
      "-rw-r--r-- 1 leo leo 128K févr. 12 22:30 'utf8_2-seg_1871-codepoints_96-dim_N-24-k3_coprimes-(3, 5, 11, 13)_cycles-(4, 6, 8, 10, 12)_sparse.npy'\n",
      "-rw-r--r-- 1 leo leo 7,3M janv. 20 12:41 'utf8_3-seg_59328-codepoints_128-dim_N-37-k4_coprimes-(11, 13, 19, 23)_cycles-(11, 7, 4, 3)_dense.npy'\n",
      "-rw-r--r-- 1 leo leo 4,5M janv. 20 12:41 'utf8_3-seg_59328-codepoints_128-dim_N-37-k4_coprimes-(11, 13, 19, 23)_cycles-(11, 7, 4, 3)_sparse.npy'\n"
     ]
    }
   ],
   "source": [
    "ls -lh codes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code creation \n",
    "\n",
    "The goal of this is to select a few elements from the given dictionary, sort all the characters and do a more elaborate codebook than the base one with the same formatting:\n",
    "\n",
    "    (codes, symbol2int, int2symbol)\n",
    "\n",
    "There is the need to check that the special codes are there, just in case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['◌',\n",
       " '◍',\n",
       " '◀',\n",
       " '◂',\n",
       " '▸',\n",
       " '▶',\n",
       " '▒',\n",
       " '\\x00',\n",
       " '\\x01',\n",
       " '\\x02',\n",
       " '\\x03',\n",
       " '\\x04',\n",
       " '\\x05',\n",
       " '\\x06',\n",
       " '\\x07',\n",
       " '\\x08',\n",
       " '\\t',\n",
       " '\\n',\n",
       " '\\x0b',\n",
       " '\\x0c',\n",
       " '\\r',\n",
       " '\\x0e',\n",
       " '\\x0f',\n",
       " '\\x10',\n",
       " '\\x11',\n",
       " '\\x12',\n",
       " '\\x13',\n",
       " '\\x14',\n",
       " '\\x15',\n",
       " '\\x16',\n",
       " '\\x17',\n",
       " '\\x18',\n",
       " '\\x19',\n",
       " '\\x1a',\n",
       " '\\x1b',\n",
       " '\\x1c',\n",
       " '\\x1d',\n",
       " '\\x1e',\n",
       " '\\x1f',\n",
       " ' ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_CODES_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charcodes = sorted(charcodes, key=lambda k: k['token'])\n",
    "chars = [k['token'] for k in charcodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1519, 1519)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars), len(set(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = []\n",
    "for c in chars:\n",
    "    counter.append((c, chars.count(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that there are no duplicates\n",
    "for c in counter:\n",
    "    if c[1]>1:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = charcodes_dict2codebook(charcodes)\n",
    "codebook, symbol2int, int2symbol = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1519, 1519, 1519)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimensions match\n",
    "len(codebook), len(symbol2int), len(int2symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1519, 187), dtype('int8'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape and datatype\n",
    "codebook.shape, codebook.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "120\n",
      "120\n",
      "60\n",
      "60\n",
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# check dimensions for each part of the code\n",
    "for i in charcodes[0].values():\n",
    "    try:\n",
    "        print(len(i))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"codes/compositional_charcode_1519_codebook_complete_conv-non_accent_sum-casing-alnum.pkl\"\n",
    "\n",
    "with open(fname, 'wb') as f:\n",
    "    pickle.dump(codes, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14M\n",
      "-rw-r--r-- 1 leo leo 205K févr. 12 22:30  adhoc-codebook-1871.pkl\n",
      "-rw-r--r-- 1 leo leo  20K févr.  7 12:12  all_chars_byline.chars\n",
      "-rw-r--r-- 1 leo leo  15K févr.  7 12:12  all_chars.chars\n",
      "-rw-r--r-- 1 leo leo 302K avril 11 16:01  compositional_charcode_1519_codebook_complete_conv-non_accent_sum-casing-alnum.pkl\n",
      "-rw-r--r-- 1 leo leo 1,3M avril 11 15:59  compositional_charcode_1519_raw_dicts.pkl\n",
      "-rw-r--r-- 1 leo leo 176K févr. 12 22:30 'utf8_2-seg_1871-codepoints_96-dim_N-24-k3_coprimes-(3, 5, 11, 13)_cycles-(4, 6, 8, 10, 12)_dense.npy'\n",
      "-rw-r--r-- 1 leo leo 128K févr. 12 22:30 'utf8_2-seg_1871-codepoints_96-dim_N-24-k3_coprimes-(3, 5, 11, 13)_cycles-(4, 6, 8, 10, 12)_sparse.npy'\n",
      "-rw-r--r-- 1 leo leo 7,3M janv. 20 12:41 'utf8_3-seg_59328-codepoints_128-dim_N-37-k4_coprimes-(11, 13, 19, 23)_cycles-(11, 7, 4, 3)_dense.npy'\n",
      "-rw-r--r-- 1 leo leo 4,5M janv. 20 12:41 'utf8_3-seg_59328-codepoints_128-dim_N-37-k4_coprimes-(11, 13, 19, 23)_cycles-(11, 7, 4, 3)_sparse.npy'\n"
     ]
    }
   ],
   "source": [
    "ls -lh codes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
