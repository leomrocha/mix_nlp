{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Compositional Codebook Preparation\n",
    "\n",
    "For this codebook I'll start from the previous idea where each codepoint was independent and only had a number id.\n",
    "\n",
    "For the new code the idea  is a bit more elaborated where:\n",
    "\n",
    "- each first iteration codepoint depends on the index\n",
    "- from this first iteration new codes are derived in the following way:\n",
    " * for the single char codes:\n",
    "   - normalize char with NFKD (so it is decomposed)\n",
    "   - check if is num, is uppercase, is_special, has_diacritic (and which)\n",
    "   - char to lowercase, char to ascii (or simplest representation)\n",
    "   - code is composed of the concatenation of (to_ascii, lowercase_code, is uppercase|lowercase, is numeric|not numeric, is special|not special, diacritic\n",
    " * for the multiple character codes:\n",
    "   - normalize sequence with NFKD\n",
    "   - encode each character\n",
    "   - conv(to_ascii ..) cat sum (to_ascii) cat conv (to lower) cat sum(to_lower) cat conv (diacritics) cat sum(diacritics) cat charcount cat hasnum, cat isnum ... (TODO, finish deciding which kind of code and what does it contains)\n",
    "\n",
    "\n",
    "The idea is:\n",
    "\n",
    "Each character representation contains more information than a simple index, this should make the network's learning easier and give a way of conversion between upper/lower with and without diacritics.\n",
    "\n",
    "The composed code gives information about the presence or absense of a character (the sums) and the order (the convolutions), this should give subspaces where is easier for similarity and proximity analysis.\n",
    "\n",
    "The issue here is that maybe each subspace part should be considered/processed in parallel while getting some information from the other subspace instead of doing it in a big neural network .... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current assumptions are the following:\n",
    "\n",
    "- Origin language is given by name not detected\n",
    "- Destination language is Given by name, not detected\n",
    "- For training a destination vector will be either checked with similarity search (FAISS) or as a one-hot encoding depending on the resource ussage\n",
    "- The input embeddings mapping will be pre-computed (as in the previous iteration) but the number of input elements will be bigger\n",
    "- The tokenization will be greedy, meaning it will try to span the longest sequences first\n",
    "- unknown input tokens should be tested with the following two encoding protocols:\n",
    "  * only span the longest tokens possible\n",
    "  * encode the entire symbol as per the compositional encoding protocol\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
