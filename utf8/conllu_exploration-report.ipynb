{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical Conllu file Exploration of  Universal Dependencies\n",
    "\n",
    "## Introduction\n",
    "\n",
    "While much work is being done in the current days on NLP and NLU, there is little work on describing why a certain length of transformer (or other as LSTM time steps) architecture has been chosen for the training, it is mostly arbitrary and depends on the goal of the work and resources available (mainly hardware). These decisions are hard once the model has been trained and there is nothing that can be done to extend the length of a transformer (for example) without having to retrain the entire network. There are however some works that tackle variable length sequences. \n",
    "\n",
    "This work presents a first complete analysis of the Universal Dependencies v2.6 dataset and presents the globan and individual results of each language present in the dataset.\n",
    "\n",
    "This work does not intend to be a conference level paper (that is why there are no references to all the papers on each subject), but an informational technical report that might help to better select the most effective compromise text or token length for your particular application.\n",
    "\n",
    "This notebook is dedicated to explore the basic text statistics (number of tokens, number of character) in the samples covered in the Universal Dependencies v2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "While doing this work I found "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The best compromise for choosing a sequence length on the NLP architecture for training will depend mostly on the requirements of the applicatino, nevertheless with the numbers here you should be able to make an informed guess on what might be better for your case.\n",
    "\n",
    "We can see that having a multi-lingual approach will necessary make the needed sequences longer as there is a large variability on sequence length, but appliying to single language might allow you to optimize your neural architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: EU Official Languages\n",
    "\n",
    "Just for information (as it happens I'm targetting mostly EU languages in my own research work)\n",
    "\n",
    "The European Union has 23 official languages:\n",
    "\n",
    "Bulgarian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hungarian, Irish, Italian, Latvian, Lithuanian, Maltese, Polish, Portuguese, Romanian, Slovak, Slovenia, Slovene, Spanish and Swedish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import math\n",
    "import os, sys\n",
    "import orjson as json\n",
    "import pyconll\n",
    "import pyconll.util\n",
    "from pycountry import languages\n",
    "\n",
    "try:\n",
    "    from utf8.utils import *\n",
    "except:\n",
    "    # to solve issue with ipython executing this import\n",
    "    from utils import *\n",
    "\n",
    "from preprocessors.preprocess_conllu import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_VERSION = \"2.6\"\n",
    "BASEPATH = \"~/projects/Datasets/text\"\n",
    "CONLLU_BASEPATH = os.path.join(BASEPATH, 'UniversalDependencies/ud-treebanks-v{}'.format(UD_VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir=CONLLU_BASEPATH\n",
    "blacklist=BLACKLIST\n",
    "allconll = get_all_files_recurse(rootdir)\n",
    "train, test, dev = filter_conllu_files(allconll, blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conllu_get_fields(fname):\n",
    "    \"\"\"\n",
    "    Processes one conllu file\n",
    "    :param fname: absolute path to the conllu file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    conll = pyconll.load_from_file(fname)\n",
    "    upos = []\n",
    "    xpos = []\n",
    "    deprel = []\n",
    "    sentences = []\n",
    "    forms = []\n",
    "\n",
    "    src_lang = path_leaf(fname).split('_')[0]\n",
    "    for sen in conll:\n",
    "        sentences.append((src_lang, sen.text))\n",
    "        try:\n",
    "            forms.extend([t.form for t in sen._tokens])\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sen_upos = [t.upos for t in sen._tokens]\n",
    "            upos.append((src_lang, sen.text, tuple(sen_upos)))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sen_xpos = [t.xpos for t in sen._tokens]\n",
    "            xpos.append((src_lang, sen.text, tuple(sen_xpos)))\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            sen_deprel = [t.deprel for t in sen._tokens]\n",
    "            deprel.append((src_lang, sen.text, tuple(sen_deprel)))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return (set(upos), len(upos)), (set(xpos), len(xpos)), (set(deprel), len(deprel)), (set(sentences), len(sentences)), (set(forms), len(forms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _try_get_2list(fname):\n",
    "    try:\n",
    "        return conllu_get_fields(fname)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing file: {} \\nWith error: {}\".format(fname, e))\n",
    "\n",
    "\n",
    "def conllu_process_get_2list(rootdir=CONLLU_BASEPATH, blacklist=BLACKLIST):\n",
    "    allconll = get_all_files_recurse(rootdir)\n",
    "    train, test, dev = filter_conllu_files(allconll, blacklist)\n",
    "    all_files = train + test + dev\n",
    "    print(all_files)\n",
    "\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        res = pool.map(_try_get_2list, all_files)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "CPU times: user 13.8 ms, sys: 20.7 ms, total: 34.5 ms\n",
      "Wall time: 81.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = conllu_process_get_2list(blacklist=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding now the shortest and longest sequences, checking the length and plotting those to see what's happening with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "upos_data = []\n",
    "xpos_data = []\n",
    "deprel_data = []\n",
    "sentences_data = []\n",
    "forms_data = []\n",
    "\n",
    "for r in res:\n",
    "    upos_val, xpos_val, deprel_val, sentences_val, forms_val = r\n",
    "#     print(\"lala 1\")\n",
    "    forms_data.extend(forms_val[0])\n",
    "    for val in upos_val[0]:\n",
    "#         print(val)\n",
    "        lang1, txt1, upos  = val\n",
    "        upos_data.append((lang1, txt1, upos, len(upos)))\n",
    "    for lang2, txt2, xpos in xpos_val[0]:\n",
    "        xpos_data.append((lang2, txt2, xpos, len(xpos)))\n",
    "    for lang3, txt3, deprel in deprel_val[0]:\n",
    "        deprel_data.append((lang3, txt3, deprel, len(deprel)))\n",
    "    for lang4, txt4 in sentences_val[0]:\n",
    "        sentences_data.append((lang4, txt4, len(txt4)))\n",
    "\n",
    "# upos_data = sorted(upos_data)\n",
    "# xpos_data = sorted(xpos_data)\n",
    "# deprel_data = sorted(deprel_data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upos = pd.DataFrame(upos_data, columns=[\"lang\", \"text\", \"upos\", \"upos_len\"])\n",
    "df_xpos = pd.DataFrame(xpos_data, columns=[\"lang\", \"text\", \"xpos\", \"xpos_len\"])\n",
    "df_deprel = pd.DataFrame(deprel_data, columns=[\"lang\", \"text\", \"deprel\", \"deprel_len\"])\n",
    "df_txt = pd.DataFrame(sentences_data, columns=[\"lang\", \"text\", \"text_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lang', 'text', 'upos', 'upos_len'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       0\n",
       "unique      0\n",
       "top       NaN\n",
       "freq      NaN\n",
       "Name: lang, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upos['lang'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = sorted(df_upos['lang'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token (by UPOS) length analysis and histogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>upos</th>\n",
       "      <th>upos_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lang text upos upos_len\n",
       "count     0    0    0        0\n",
       "unique    0    0    0        0\n",
       "top     NaN  NaN  NaN      NaN\n",
       "freq    NaN  NaN  NaN      NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_upos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    fig, ax = plt.subplots()\n",
    "    dest_lang = languages.get(alpha_2=lang) if len(lang) == 2 else languages.get(alpha_3=lang)\n",
    "    dest_lang = dest_lang.name\n",
    "    ax.set_title(lang +\":\" +dest_lang )\n",
    "    df_upos.loc[df_upos['lang'] == lang]['upos_len'].hist(bins=100, ax=ax, label=lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A token is a word or a punctuation mark. Punctuation is counted in the sentence length.\n",
    "\n",
    "Is interesting to see that most languages have few sentences longer than 100 tokens (including the punctuation)\n",
    "\n",
    "Also, each language has a different centroid for the sentence length.\n",
    "\n",
    "Also it's an interesting that different languages show different strip patterns, each having a different interval of lengths that are empty (like basque for example) while the count next to it has high counts (length 10 for basque has a count of more than 600 while the left and right are just empty). \n",
    "\n",
    "These patterns are really interesting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>deprel</th>\n",
       "      <th>deprel_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lang text deprel deprel_len\n",
       "count     0    0      0          0\n",
       "unique    0    0      0          0\n",
       "top     NaN  NaN    NaN        NaN\n",
       "freq    NaN  NaN    NaN        NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deprel.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    fig, ax = plt.subplots()\n",
    "    dest_lang = languages.get(alpha_2=lang) if len(lang) == 2 else languages.get(alpha_3=lang)\n",
    "    dest_lang = dest_lang.name\n",
    "    ax.set_title(lang +\":\" +dest_lang )\n",
    "    df_deprel.loc[df_deprel['lang'] == lang]['deprel_len'].hist(bins=100, ax=ax, label=lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character length analysis and histogram plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lang text text_len\n",
       "count     0    0        0\n",
       "unique    0    0        0\n",
       "top     NaN  NaN      NaN\n",
       "freq    NaN  NaN      NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the values we can see, even though we have a  max of 3471 characters, if we want to see how much of the spectrum we capture is as follows (if it were a normal distribution which I don't want to test yet but it should work well enough):\n",
    "\n",
    "| char_len | mean+X*std | % captured |\n",
    "|:--------:|:----------:|:----------:|\n",
    "|    90    |            |     50%    |\n",
    "|    138   |            |     75%    |\n",
    "|    173   |  103+1*70  |    84.2%   |\n",
    "|    243   |  103+2*70  |    98.8%   |\n",
    "|    313   |  103+3*70  |    99.9%   |\n",
    "\n",
    "So going for a maximum sequence length of at least 313 should capture most of the sequences in the training and testing datasets (and as they should also be significant of each language ... it should be enough). This is done for each language later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "len_lang_list = []\n",
    "\n",
    "_99p = []\n",
    "_98p = []\n",
    "_84p = []\n",
    "\n",
    "for lang in langs:\n",
    "    dest_lang = languages.get(alpha_2=lang) if len(lang) == 2 else languages.get(alpha_3=lang)\n",
    "    dest_lang = dest_lang.name\n",
    "    lng_txt = df_txt.loc[df_txt['lang'] == lang]\n",
    "    d = lng_txt.describe()\n",
    "    mean = d.loc['mean']['text_len']\n",
    "    std = d.loc['std']['text_len']\n",
    "    ef,ne,nn = math.ceil(mean+std), math.ceil(mean+2*std), math.ceil(mean+3*std)\n",
    "    _99p.append(nn)\n",
    "    _98p.append(ne)\n",
    "    _84p.append(ef)\n",
    "    len_lang_list.append((lang, dest_lang, ef, ne, nn))\n",
    "    print(dest_lang)\n",
    "    print(\"\"\"\n",
    "            |    {}   |  84.2%   |\n",
    "            |    {}   |  98.8%   |\n",
    "            |    {}   |  99.9%   |\"\"\".format(ef, ne, nn)\n",
    "    )\n",
    "    print(lng_txt.describe())\n",
    "    print(\"_\"*50)\n",
    "#     lng_txt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ba5ea7e30bdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_99p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_98p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_84p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "max(_99p), max(_98p), max(_84p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking individually each language, the maximum length would be much higher, this is because some languages contain longer sentences, so we have to deal with this selecting a bigger sentence length to be able to capture most of it.\n",
    "\n",
    "The longest being Belarusian\n",
    "\n",
    "The complete list is sorted and printed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(reversed(sorted(len_lang_list, key=lambda x: x[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    fig, ax = plt.subplots()\n",
    "    dest_lang = languages.get(alpha_2=lang) if len(lang) == 2 else languages.get(alpha_3=lang)\n",
    "    dest_lang = dest_lang.name\n",
    "    ax.set_title(lang +\":\" +dest_lang )\n",
    "    df_txt.loc[df_txt['lang'] == lang]['text_len'].hist(bins=100, ax=ax, label=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAYBE_BLACKLIST_LANGS = ['ceb', 'jv', 'ce', 'cv', 'dv', 'ht', 'hy', 'ku', 'mh', 'mi', 'ps', 'su', 'tk', 'ba', 'tg',\n",
    "                         'tt', 'ug'\n",
    "                         ]\n",
    "# blacklisting due to lack of samples, extinct language or other issue\n",
    "EXTRA_BLACKLIST = [\"olo\", \"swl\", \"bxr\", \"fa\", \"sme\", \"aii\", \"gun\", \"yo\", \"akk\", \"fo\", \"mdf\",\n",
    "                   \"krl\", \"pcm\", \"bho\", \"sms\", \"am\", \"bm\", \"got\", \"cu\", \"hsb\", \"wo\",\n",
    "                  ]\n",
    "\n",
    "# blacklists to reduce even more the number of languages, latin is left because will be used FIRST to train to set a learning baseline ...\n",
    "# base, greek, old french and ancient greek will be nice too if I manage to transliterate it\n",
    "# extra blacklisting to reduce the number of alphabets used and other low resource and other non-official languages\n",
    "MORE_EXTRA_BLACKLIST = [\"af\", \"gsw\", \"he\", \n",
    "                        \"ca\", \"cy\", \"eu\", \"ga\", \"gd\", \"gl\", \"cr\", \"hy\", \"tr\",\n",
    "                       ]\n",
    "\n",
    "ANCIENT_LANGS = [\"la\", \"grc\", \"fro\",]  # latin, ancient greek, old french -> base for MANY languages\n",
    "GREEK_BLACKLIST = [\"el\", \"grc\"]\n",
    "\n",
    "# this means mainly taking out cyrillic scripts ... but bulgarian IS in the EU and uses \n",
    "# cyrillic so ... there it is, won't take them out\n",
    "CYRILLIC_BLACKLIST = [\"be\", \"bg\", \"he\", \"ru\", \"sr\", \"uk\"]\n",
    "\n",
    "\n",
    "BLACKLIST_LANGS = ['ar', 'as', 'arz', 'azb', 'bn', 'bp', 'ckb', 'eo', 'ew', 'fa', 'fo', 'gom', 'gu', 'hi', 'hu', 'id',\n",
    "                   'ilo', 'ja', 'ka', 'kk', 'ko', 'lmo', 'ml', 'mr', 'mwl', 'ne', 'pa', 'py', 'sh', 'si', 'ta', 'te',\n",
    "                   'th', 'tl', 'ur', 'vi',\n",
    "                   'wuu', 'yi', 'zb', 'zh'\n",
    "                   ] + MAYBE_BLACKLIST_LANGS + EXTRA_BLACKLIST\n",
    "\n",
    "BLACKLIST_LANGS = sorted(list(set(BLACKLIST_LANGS)))\n",
    "\n",
    "len(BLACKLIST_LANGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, sample count and some other observations, I'm now cutting more languages (the EXTRA_BLACKLIST) such as to cut the complexity of the training dataset while trying to keep as much as possible to make a multi lingual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def show_len_by_lang(column='upos_len', x=10):\n",
    "# #     return df_upos[df_upos[column == x]]  # ['upos_len'].hist(bins=100, log=True)\n",
    "#     return df_upos.loc[df_upos[column] > x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deprel['deprel'].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# all_upos = set([])\n",
    "# all_upos_count = 0\n",
    "# all_deprel = set([])\n",
    "# all_deprel_count = 0\n",
    "\n",
    "# for r in res:\n",
    "#     (upos, upos_count), (xpos, xpos_count), (deprel, deprel_count) = r\n",
    "#     all_upos = all_upos.union(upos)\n",
    "#     all_upos_count += upos_count\n",
    "#     all_deprel = all_deprel.union(deprel)\n",
    "#     all_deprel_count += deprel_count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_upos), all_upos_count, len(all_deprel), all_deprel_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems kind of big for putting it directly in a memory, some compression should be done if this method is to work ... (it should be more data efficient than the current methods)\n",
    "\n",
    "For a single language that might be feasible, but as more languages are added this leads to a volume problem ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lall_upos = list(all_upos)\n",
    "lup = [len(u) for u in lall_upos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lup), min(lup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lup.index(max(lup)), lup.index(min(lup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lall_upos[83026]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lall_upos[371226]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue here seems to be the length of the sentences, putting a max length might work but will leave longer sentences out of the training.\n",
    "\n",
    "The other idea would be to use a more thourough diccionary that contains more elements this will make smaller sentences, this means, for the training set the max length is 515 words, this is still too much.\n",
    "\n",
    "The counterpoint when using a bigger dictionary is that even if the length of the input is smaller, the bigger the dictionary  the bigger the memory impact for the encoding and decoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I should get all the words and check the number and length there are\n",
    "\n",
    "len(forms_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
