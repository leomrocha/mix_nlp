{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical Analysis of more than 50000 Books from Project Gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "With the advent of deep neural networks, growing computing power and improvements in techniques (as Transformers networks) there is a growing interest in Natural Language Processing (NLP) and Natural Language Understanding (NLU) on the scientific comunity, this has also led to more practical use cases in industry and libraries such as [Rasa](TODO) and [SpaCy](TODO) have arised.\n",
    "\n",
    "In the scientific comunity the most recent tendency is to construct big transformer networks with billions of parameters and put them through enormous unsupervised learning datasets. As an example the latest [GPT-3 model](https://arxiv.org/abs/2005.14165) contains 175 Billion parameters, a single trainig run has been estimated to cost [4.6M USD](https://lambdalabs.com/blog/demystifying-gpt-3/#:~:text=%244%2C600%2C000%3A%20The%20full%20cost%20of,per%2Dhour%20GPUs%20on%20market.) on commercially available hardware. \n",
    "\n",
    "From the practical point of view, these networks are impossible to train and use in a commercial setup due to several problems, including training and operation costs as well as latency and throughput of these networks. This is the reason software as SpaCy use a more manual approach having lookup tables to the first stages of text processing. Nevertheless there is always in today's setup a deep neural network layer, be a transformer, CNN, or a variation of RNN, being the most popular of the Recurrent NNs the LSTM architecture.\n",
    "\n",
    "From the design point of view, a network must be designed to fit the use cases, which in part means to fit the inputs, for this purpose statistics and knowledge about the domain are necessary. The information provided in this study is intended to help in that decision.\n",
    "\n",
    "From our knowledge, even if there are some [simple statistics available](http://www.gutenbergnews.org/statistics/) this is the first work that presents a thorough statistical analysis on (most of) the books present in [Project Gutenberg](https://www.gutenberg.org/) at the date of 30st May 2020. \n",
    "\n",
    "This work is intended as a help for lingüists and scientist working with language data.\n",
    "\n",
    "Note that this is a continuation of the previous work [A Statistical Exploration of Universal Dependencies Conllu Files](TODO) by the [same author](TODO me) on statistic analysis of NLP and lingüistic datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Results Files\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org/) books up to 30th May 2020. This is TODO GB\n",
    "[Project Gutenberg Catalog](https://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs) to be able to get metadata on each book instead of having to find it and parse it from the text.\n",
    "\n",
    "\n",
    "The original data:\n",
    "* 16 GB compressed (zipped)\n",
    "* 91793 (zipped) books, many books contain more than one format\n",
    "* 59897 (zipped) unique books\n",
    "* RDF Database Catalog 1.3 GB\n",
    "\n",
    "There are many books that are not presented in the results, this is due to errors during processing or book size which limited c\n",
    "\n",
    "The results files are:\n",
    "* ID.stats.json.gz -> global statistics about the file ID\n",
    "* ID.stats_all.json.gz -> complete statistics including count for every paragraph, sentence and token length and word usage\n",
    "* Total of 8.0 GB of compressed data.\n",
    "\n",
    "Note: If new data is added the process is the same for any new book, but the global per language statistics will need to be recomputed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
