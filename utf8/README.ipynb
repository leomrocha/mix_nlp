{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comprehensive Study of Multiple Encoding Techniques on Low Resource Embeddings for Character-Level NLP\n",
    "\n",
    "## Flexible Universal Character Level Encoding method for Text in NLP\n",
    "\n",
    "    Leonardo M. Rocha\n",
    "    leo <dot> m <dot> rocha <at> gmail <dot> com\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Currently NLP deals with *Out of Vocabulary* (OOV) in different ways, this leads to several non-necessarilly efficient ways of pre-processing NLP datasets to be able to deal with \n",
    "In this work we present **Segment-Multihot-Encoding** (SME) a technique that deals with OOV and allows to encode all possible symbols in a computationally efficient manner to encode all (or part) of the UTF-8 domain in a fixed multi-hot encoding that can be further compressed by overfitting. This technique eliminates the need of complex and compute consuming pre-processing replacing it for a much more simple one that works for *any* dataset. This work focuses on being able to encode a symbol, as once it is encoded the network can be fine-tunned later to handle previously unseen ones.\n",
    "\n",
    "This work presents the SME technique for UTF8 and we call it **SME-UTF8**, the source code and encoding vectors are also given with usage examples.\n",
    "\n",
    "There is an extra advantage of this methodology is that with deterministic and defined process with small representation size can be implemented efficiently in hardware.\n",
    "\n",
    "This is also presented directly as an ipython notebook to also be Executable in the places that is needed. We call this, the Executable Paper and the idea is to improve reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Related Work\n",
    "\n",
    "Currently for NLP tasks there is the need to first analyze the input domain and encode it to deal with *Out of Vocabulary* (OOV) words or symbols and Polysemy. \n",
    "\n",
    "It is important to separate (and we do in this work) the encoding part (to be able to represent the symbol) from the learning to use those symbols (the network to be able to do something useful with it) as this work focuses solely on being able to encode all feasible symbols in a defined text encoding domain. In this case the work is done for UTF-8 which is the most used text encoding in the web [94.6% according to w3techs](https://w3techs.com/technologies/details/en-utf8).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Diverse techniques deal differently with OOV, ranging from techniques that can not deal with them, like [GloVe - Pennington et al. 2014](https://nlp.stanford.edu/pubs/glove.pdf) or [Word2Vec - Milikov et al. 2013](https://arxiv.org/abs/1301.3781) to others such as [Universal Sentence Encoder -Cer et al. 2018](https://arxiv.org/abs/1803.11175) or the one for FastText can encode OOV with subword embeddings\n",
    "One of the most used techniques is [Byte Pair Encoding from Neural Machine Translation of Rare Words with Subword Units Sennrich et al. 2015](https://arxiv.org/abs/1508.07909) which has the advantage of compressing the input size hence accelerate training compared to a full character level input on the current SoTA.\n",
    "\n",
    "\n",
    "[ELMo - Peters et al. 2018](https://arxiv.org/abs/1802.05365)\n",
    "\n",
    "[Transformer - ]()\n",
    "[ULM-FiT - ]()\n",
    "[BERT - ]()\n",
    "[AlBERT - ]()\n",
    "[CamemBERT]()\n",
    "\n",
    "\n",
    "... TODO more papers and references here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All current methods deal with subdomains of the possible inputs available, which for most tasks is enough, nevertheless the weakness is that they can not deal with **all** posisble input symbols, which for the current study is UTF-8.\n",
    "\n",
    "In the case of continual learning the need to add new symbols is a given, be it due to adding new domain in the same language, or new languages\n",
    "\n",
    "The goal of this work is to try to set a more standard way to deal with all possible symbols in a defined encoding standard.\n",
    "\n",
    "This work analyzes UTF-8 encoding and presents a technique to be able to encode part or all of the UTF-8 domain in an computationally efficient way. The same technique can be used for other text encodings without any modification, and as utf-8 is a superset of other encodings (ASCII, Unicode, ...) the same matrix encoding can be applied without any modification in those datasets.\n",
    "\n",
    "There is also another point to say about this computational complexity, all the current SoTA methods are trained in clusters (and wiht prices) that are unavailable to most users. The current work is part or a larger work on trying to get enough permormance in commercially available (and relatively accessible) single GPUs for end users (being at the current time the NVidia RTX2080ti one of the more computationally strong cards in the market).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections\n",
    "\n",
    "This paper deals first with an analysis of the UTF-8 encoding\n",
    "\n",
    "Then goes to deal with the construction of the multi-hot code proposed\n",
    "\n",
    "After works on compressing the multi-hot code with Overfitting (yes, OVERFITTING)\n",
    "\n",
    "Then goes to the evaluation of the codes and compare results with other encoding methods\n",
    "\n",
    "Finally goes to the conclussion and future work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTF-8 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding\n",
    "\n",
    "[One-hot encoding](https://en.wikipedia.org/wiki/One-hot) is one of the most used to encode cathegorical variables, in the case of State of the Art NLP tasks is used to encode the input symbols, this is computationally expensive and the goal here is to reduce this complexity leaving memory and computational space for other more complex tasks in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of code-points\n",
    "\n",
    "As this file tries to encode all the characters possible by utf-8 we have to check the feasible number so:\n",
    "\n",
    "From [Wikipedia utf-8](https://en.wikipedia.org/wiki/UTF-8)\n",
    "\n",
    "UTF-8 is a variable width character encoding capable of encoding all **1,112,064**.\n",
    "\n",
    "$$ 17×2^{16} = 1114112 $$ code points minus 2,048 technically-invalid surrogate code points\n",
    "\n",
    "This is, if encoding with one-hot we would need 1.1M parameters per neuron in the input layer, which is expensive. The goal is to reduce this complexity (which we argue is unnecessary) by orders of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTF-8 structure and Encoding Details\n",
    "\n",
    "Since the entire utf-8 univers is NOT the entire $2^{32}$ domain, but there are limitations explained in [the utf-8 description](https://en.wikipedia.org/wiki/UTF-8)\n",
    "\n",
    "| Number of bytes | Bits for code point | First code point | Last code point | Byte 1   | Byte 2   | Byte 3   | Byte 4   |\n",
    "|----------------|--------------------|-----------------|----------------|----------|----------|----------|----------|\n",
    "| 1              | 7                  | U+0000          | U+007F         | 0xxxxxxx |          |          |          |\n",
    "| 2              | 11                 | U+0080          | U+07FF         | 110xxxxx | 10xxxxxx |          |          |\n",
    "| 3              | 16                 | U+0800          | U+FFFF         | 1110xxxx | 10xxxxxx | 10xxxxxx |          |\n",
    "| 4              | 21                 | U+10000         | U+10FFFF       | 11110xxx | 10xxxxxx | 10xxxxxx | 10xxxxxx |\n",
    "\n",
    "The UTF-8 code is formed by 4 segments, we will refer to this often during the current work.\n",
    "\n",
    "The thing is that the number of elements in the table should be at most $2^{21}$, There is only the need to create a index that can handle the 4 cases which can be done with 4 different conversion tables.\n",
    "\n",
    "\n",
    "In fact it is possible to just cut the utf-8 value in chunks and do one-hot per different parts:\n",
    "- there are only 4 segment ranges, that can be coded to add redundancy in one-hot also add there either hamming or other ECC\n",
    "- the largest value is for 8 bits -> 256 values\n",
    "- the others contain 6 bits -> 64 values\n",
    "The prefix of each can be taken away and replaced by the initial one-hot\n",
    "\n",
    "So a complete code would be:  $ 4 + 256 + 64 + 64 + 64 = 324 $\n",
    "\n",
    "Instead of having a vector of dimension 1,112,064 to encode any utf-8 value, one with dimension 452 would be able to encode everything in the utf-8 domain.\n",
    "\n",
    "This embedding can stil be reduced but should be sparse enough already to make a good input, the goal here is to have sparse vector that makes each vector far enough of the others, at least by one dimension. Adding the redundancy code (the first 4 dimensions) allows to make distance even bigger for vectors that should be further appart taking into account the locality of the utf-8 encoding (each character set is close to the ones used with it, segment 3 encodes mostly CJK Chinese-Japanese-Korean).\n",
    "\n",
    "#### Notes\n",
    "It is worth noting here that the first author has also experience in communications which allowed during the curse of this research the analysis of multiple Error Correcting Codes (ECCs) and different kinds of encoding (for example encoding as a Fourier Series), the conclusion is that even if the one-hot is the best in distance, other codes can be used and a multi-hot sparse is the simplest to implement (and fastest to encode). As a note, one pending task is to analyze ECCs in an end to end manner for a neural network. Some of these analysis (without proper formating) can be found in the anex notebooks folders of this repository or at [text subfolder in the minibrain project](https://github.com/leomrocha/minibrain/tree/master/predictors/sequence/text) where most of the experimental code is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8 Segments\n",
    "To cut even more memory consumption the table can be generated for 1-4 segments of the utf-8 code, taking into account that the 4th segment is mostly composed of:\n",
    "* Supplementary Multilingual Plane (SMP) of historic scripts\n",
    "* Supplementary Special-purpose Plane (SSP)\n",
    "* Private Use Areas (SSU)\n",
    "* Invalid Codes\n",
    "\n",
    "We can safely ignore this 4th segment (for the purpose of this article and most usages) which adds to most of the code-points\n",
    "\n",
    "If CJK, Indic and some Miscelaneous Symbols are not (and will not be) needed then the 3rd segment can be safely ignored too reducing even more the memory consumption of the application\n",
    "\n",
    "So the result would be:\n",
    "\n",
    "| Segment | # of code points | First index | Last index | Vector Size | # code exceptions | Size (MB) |Matrix Size (MB) | Sparse Size (MB) |\n",
    "|---------------|-----------------------|-------------|-------------|------------|-------------------|-------------------|-----------|-----------|\n",
    "| 4             | 1107904             | 61440        | 1107904     | 452 | 790656  | 11538.59   | 3820.59 | 83.59 |\n",
    "| 3             | 59328               | 2112         | 61439       | 388 | 4224    | 530.71     | 175.62  | 3.59  |\n",
    "| 2             | 1984                | 128          | 2111        | 324 | 128     |  14.84     | 4.90    | 0.09  |\n",
    "| 1             | 128                 | 0            | 127         | 260 | 0       |  0.77      | 0.25    | 0.005  |\n",
    "\n",
    "Where:\n",
    "* Segment: number of segments used from utf-8 to generate the code\n",
    "* \\# of Code Points: The total encoded code points generated\n",
    "* Vector Size: The embedding vector size\n",
    "* First / Last Index: corresponds to the segment first and last index in the embedding matrix\n",
    "* \\# Code Exceptions: Number of code exceptions during encoding with Python, notice that we use the standard library for this.\n",
    "* Size (MB): Size of the embedding matrix and conversion dictionaries (from-to code) once saved in disk in Dense mode\n",
    "* Matrix Size (MB): Size of the embedding matrix in disk in Dense Mode\n",
    "* Sparse Size (MB): Size of the embedding matrix in disk in Sparse mode\n",
    "\n",
    "\n",
    "Notice that the code for 1 segment corresponds to one-hot encoding of ASCII encoding (plus the vector of size 4 that we don't modify in any case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signaling the Start and End of a Sequence \n",
    "\n",
    "To this end we can use the codes available in the utf-8 and add the mapping to the encoding and decoding dictionaries (not the matrix), The first 0x20 codes in UTF-8 signal different communication an control codes, we can use those same for our NLP purposes or we could choose the invalid codes at 0xC0 and 0xC1 or codes larger than U+10FFFF (at the end of segment 4 as per in our vocabulary for this paper).\n",
    "\n",
    "In order to avoid any issues and as we don't count on using the current models for communication protocols but only for NLP purposes (this encoding could also be used for allowing the network to deal directly with network communication protocols) we decide to use the control codes at the beginning of the block of the first segment\n",
    "\n",
    "To this end there are 3 codes that are re-mapped by design and signaled by the following:\n",
    "* \\<start\\>: 0x02 - STX Start of Text\n",
    "* \\<end\\>: 0x03 - ETX End of Text\n",
    "* \\<unk\\>\": 0x15 - NAK  Negative Acknowledge\n",
    "\n",
    "The \\<unk\\> (Unknown) element should not be used as per design there should be no unrepresented symbols in the code design, but is added for completion and in case of future use.\n",
    "\n",
    "The mapping is created in a way to be as close as possible to the original meaning of the control symbols.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes Creation\n",
    "\n",
    "This section is dedicated to code execution and measurements to fill the table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of codes =  128\n",
      "number of code_exceptions =  0\n",
      "number of codes =  1984\n",
      "number of code_exceptions =  128\n",
      "number of codes =  59328\n",
      "number of code_exceptions =  4224\n",
      "number of codes =  1107904\n",
      "number of code_exceptions =  790656\n",
      "| Segments | exec_time (sec) |  matrix_shape | Size in Disk (MB): | Matrix Size in Disk (MB):            | Sparse Matrix Size in Disk (MB): |code path\n",
      "| 1 | 0.005 | (128, 260) | 0.78 | 0.25 | 0.00 | codes/utf8_codes-1seg.pkl |\n",
      "| 2 | 0.052 | (1984, 324) | 14.85 | 4.90 | 0.09 | codes/utf8_codes-2seg.pkl |\n",
      "| 3 | 2.070 | (59328, 388) | 530.71 | 175.62 | 3.59 | codes/utf8_codes-3seg.pkl |\n",
      "| 4 | 38.599 | (1107904, 452) | 11538.59 | 3820.59 | 83.59 | codes/utf8_codes-4seg.pkl |\n"
     ]
    }
   ],
   "source": [
    "from paper_main import create_measure_tables\n",
    "create_measure_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "### Execution of previous results\n",
    "\n",
    "The execution of all the previous code is done in a single thread of an Intel i7 7700.\n",
    "\n",
    "#### Embedding Sizes\n",
    "\n",
    "Size of the embedding matrix grows with the number of code points and embedding size, observing the size of the matrices, the sparse representation of them is negligeable in comparison with current models. \n",
    "\n",
    "NVidia provides support for sparse operations with [cuSPARSE ](https://developer.nvidia.com/cusparse) ([documentation](https://docs.nvidia.com/cuda/cusparse/index.html)) which means we can use these matrices. \n",
    "\n",
    "Nevertheless many applications work in dense mode and if is the case working with less than the 4 segments would be advisable. The 3 segments embedding provides enough representation for all languages on earth and the 2 segment one support most languages already as stated in a previous section.\n",
    "\n",
    "#### Execution Time\n",
    "\n",
    "As seen in the code execution above, even if vectors are big to keep saved and download, the creation of the vectors is defined and can be recreated with less than a minute execution in a single thread of an off-the-shelf CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC Configuration:\n",
    "\n",
    "#### Hardware\n",
    "\n",
    "    intel i7 7700\n",
    "    64GB of RAM\n",
    "    GPU-0 GTX1080 -> runs the GUI and other tasks, sometimes used for train and testing\n",
    "    GPU-1 RTX2080ti -> only used for computation\n",
    "\n",
    "#### Software\n",
    "\n",
    "    Cuda V10.1\n",
    "    Pytorch V1.3\n",
    "    Python 3.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Compression\n",
    "\n",
    "In the literature overfitting is an evil creature, but in this case, as we know the entire domain, we are going to use it to our advantage with overfitting the sparse input (the multi-hot encoded vector) into a smaller embedding vector than the input, the goal is lossless compression here.\n",
    "\n",
    "This is done to be able to make a more informed decision at the end of the study and show the comparative results.\n",
    "\n",
    "Once the network is trained (basically an overfitted autoencoder), a new encoding matrix is generating making each element of the input domain pass by the autoencoder and getting the latent vector, which is used to generate a matrix that can be given as Embedding directly to the network.\n",
    "\n",
    "As the UTF-8 coding uses a max of 4 bytes for the code representation, there is at least the need to use vectors of size 32. The best code for this would be directly using the utf-8 code as the embedding (which we should also test as input to be able to compare the results)\n",
    "\n",
    "But then, any vector representation that handles the complete domain must be at least of dimension 32, here we'll test several dimensions for each number of codes, the representation of 1 segment coding is just done for completion, the one for 2 segments coding might be useful but having only 1984 elements a one-hot encoding does not pose a big problem with current resources, the code starts to be more interesting for 3 segment and 4 segment coding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is done with the following configuration:\n",
    "\n",
    "    Batch Size: Size of all the simbols, each batch contains once every symbol in the domain\n",
    "    Network Configuration: Autoencoder\n",
    "    Loss: CrossEntropy\n",
    "    \n",
    "And we measure:\n",
    "\n",
    "    Output Vector Embedding Size\n",
    "    Execution Time\n",
    "    Matrix Size on Disk (here only the dense matrix is taken into account as there should be close to no sparsity)\n",
    "    \n",
    "\n",
    "The experiments on overfitting were run on different vector sizes from 32 to 128 for encodings using 3 and 4 segments\n",
    "\n",
    "**TODO do the runs again (with a better and more clean script) and put here the results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we develop a methodology and different codes with different vector sizes and distances. All the generated codes are saved and later used to test in NLP tasks.\n",
    "\n",
    "Two different ideas are used to create the sparse codes, the first one is choose k from N, the second idea is to generate a multi-hot with the combination of different smaller codes of co-prime sizes, this leads to longer cycles on the combination of the codes\n",
    "\n",
    "Although any code order might be enough we look for repeatable, predictable and deterministic process to reach to the same values each time we recompute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Codes, Choosing *k* of *N*\n",
    "\n",
    "For completion, this study also deals with different sparse coding techniques, the basic idea will be choosing the \n",
    "\n",
    "We need to basically do the following: ${N\\choose k}$ \n",
    "\n",
    "Where $ 32 <= N <= M$ and defining $M$ as the maximum value of the desired vector dimension \n",
    "\n",
    "and $ k $ should be minimized to augment the sparcity of the vector as much as possible\n",
    "\n",
    "Also We can again add some redundancy as in the previous multi-hot code. i.e. the first 4 elements should indicate which of the UTF-8 segment are being used for the code-point.\n",
    "\n",
    "${N\\choose k} = \\frac{(n)!}{k!(n-k)!} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector sizes are explored here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paper_main import sparse_Nk_dimension_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.09 ms, sys: 477 µs, total: 9.57 ms\n",
      "Wall time: 9.51 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "results = sparse_Nk_dimension_analysis()\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128, 136, 17, 2, '0.118'),\n",
       " (128, 165, 11, 3, '0.273'),\n",
       " (128, 210, 10, 4, '0.400'),\n",
       " (128, 252, 10, 5, '0.500'),\n",
       " (128, 210, 10, 6, '0.600'),\n",
       " (1984, 2016, 64, 2, '0.031'),\n",
       " (1984, 2024, 24, 3, '0.125'),\n",
       " (1984, 2380, 17, 4, '0.235'),\n",
       " (1984, 2002, 14, 5, '0.357'),\n",
       " (1984, 3003, 14, 6, '0.429'),\n",
       " (59328, 59640, 72, 3, '0.042'),\n",
       " (59328, 66045, 37, 4, '0.108'),\n",
       " (59328, 65780, 26, 5, '0.192'),\n",
       " (59328, 74613, 22, 6, '0.273'),\n",
       " (1107904, 1125180, 190, 3, '0.016'),\n",
       " (1107904, 1150626, 74, 4, '0.054'),\n",
       " (1107904, 1221759, 45, 5, '0.111'),\n",
       " (1107904, 1344904, 34, 6, '0.176')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results: (code points needed, possible code points, vector size, number of  ones in code, sparsity ratio)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the size of the vectors on these representation are much smaller than the manual one-hot code by segment created before. We can again use the same technique as in the first part of adding first 4 vector elements that represent the segment to which the symbol belongs.\n",
    "\n",
    "There is another point too to take into account, the size of the vector is important as hardware is more adapted for certain sizes, it also has consequences on the kind of techniques can be done, for example groups convolution.\n",
    "\n",
    "It is convenient then to have vector sizes of powers of two and also multiple of 96 (tensor operation sizes in NVidia tensor cores are of size 96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Codes, multiple joint co-prime codes\n",
    "\n",
    "The idea here is to create multiple ohe-hot codes, each code of a prime size (which is the simplest way to choose co-primes), then joining the codes.\n",
    "\n",
    "Again we look for combinations that minimize the vector size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 269 ms, sys: 0 ns, total: 269 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from paper_main import multihot_primes\n",
    "_, codes_1seg, codes_2seg, codes_3seg, codes_4seg= multihot_primes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest codes that can handle the needed codebook sizes are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((2, 3, 5, 7), 4, 0.235, 17, 210),\n",
       " ((3, 5, 11, 13), 4, 0.125, 32, 2145),\n",
       " ((11, 13, 19, 23), 4, 0.061, 66, 62491),\n",
       " ((23, 31, 37, 43), 4, 0.03, 134, 1134383))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_1seg[0], codes_2seg[0], codes_3seg[0], codes_4seg[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the codes, for each code we pad with redundancy such as we get an embedding vector of a size in $[32, 48, 64, 96, 128, 192, 256, 384]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also look for a defined sparsity in the final vector embeddings, as extra property, all the vectors have the same parity which is a nice to have to be able to check during decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take those two elements into account we'll select from the smallest vector sizes, we can see some of these here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([((2, 3, 5, 7), 4, 0.235, 17, 210),\n",
       "  ((3, 5, 11), 3, 0.158, 19, 165),\n",
       "  ((2, 5, 13), 3, 0.15, 20, 130),\n",
       "  ((2, 7, 11), 3, 0.15, 20, 154),\n",
       "  ((3, 5, 13), 3, 0.143, 21, 195)],\n",
       " [((3, 5, 11, 13), 4, 0.125, 32, 2145),\n",
       "  ((2, 7, 11, 13), 4, 0.121, 33, 2002),\n",
       "  ((3, 5, 7, 19), 4, 0.118, 34, 1995),\n",
       "  ((3, 7, 11, 13), 4, 0.118, 34, 3003),\n",
       "  ((3, 5, 11, 17), 4, 0.111, 36, 2805)],\n",
       " [((11, 13, 19, 23), 4, 0.061, 66, 62491),\n",
       "  ((11, 13, 17, 29), 4, 0.057, 70, 70499),\n",
       "  ((11, 17, 19, 23), 4, 0.057, 70, 81719),\n",
       "  ((7, 13, 23, 29), 4, 0.056, 72, 60697),\n",
       "  ((7, 17, 19, 29), 4, 0.056, 72, 65569)],\n",
       " [((23, 31, 37, 43), 4, 0.03, 134, 1134383),\n",
       "  ((23, 29, 37, 47), 4, 0.029, 136, 1159913),\n",
       "  ((23, 29, 41, 43), 4, 0.029, 136, 1175921),\n",
       "  ((17, 37, 41, 43), 4, 0.029, 138, 1108927),\n",
       "  ((19, 29, 43, 47), 4, 0.029, 138, 1113571)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_1seg[:5], codes_2seg[:5], codes_3seg[:5], codes_4seg[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to fill the remaining space we can choose to combine co-prime codes with N choose k. This has the advantage of generating different patterns with redundant information that can be exploited by the network later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Generation\n",
    "\n",
    "First we generate the codes for sparse and co-prime methods without filling for the HW optimization size, then we do the filling. All codes are saved to be able to experiment later with them and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paper_main import sparse_code_Nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 687 ms, sys: 21 ms, total: 708 ms\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "sc1seg = sparse_code_Nk(128, 17, 2) + np.array(range(128)).reshape([128,1]) * 17\n",
    "sc2seg = sparse_code_Nk(1984, 24, 3) + np.array(range(1984)).reshape([1984,1]) * 24\n",
    "sc3seg = sparse_code_Nk(59328, 37, 4) + np.array(range(59328)).reshape([59328,1]) * 37\n",
    "sc4seg = sparse_code_Nk(1107904, 45, 5) + np.array(range(1107904)).reshape([1107904,1]) * 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc1 = np.zeros([128,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.put(sc1, sc1seg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<128x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 240 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(sc1seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<128x2 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 240 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_matrix(sc1seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "\n",
    "Decoding the vectors can be done by cosine similarity (or any other method of vector similarity), in this case we use [faiss](https://github.com/facebookresearch/faiss) library from Facebook AI Research\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Validation\n",
    "\n",
    "To validate this method is sufficient to show that the performance of a network does not decay with compared to one-hot in several tasks. This article deals with this in a restricted environment\n",
    "\n",
    "There are a few key points to measure:\n",
    "\n",
    "* Pre-processing time (dataset) for each method\n",
    "* Network Performance \n",
    "* Network Size (total and trainable parameters)\n",
    "* Network Memory Consumption\n",
    "* Network Training Time\n",
    "\n",
    "To this end simple enough NLP tasks will be tackled such as the training and testing time is not excesive (running on a single end user graphic GPU card, in this case an RTX2080ti).\n",
    "\n",
    "The tasks will be evaluated on the same (except of the first embedding layer) networks with different encoding, to be able to compare networks all will be done at character level. For completeness some otehr methods also will be evaluated, mainly BPE which is currently used in most SoTA papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "The TODO here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclussion\n",
    "\n",
    "This work shows a different take of the current approach in how to deal with input coding for Natural Language Processing tasks on Deep Learning Networks. \n",
    "\n",
    "This work shows that is possible to reduce computational complexity *and* add representational capability to a deep neural network without loss of performance and making pre-processing easier .... BLBLBLABLABLABLABLABLABLABLABLA TODO here.\n",
    "\n",
    "## Future Work:\n",
    "\n",
    "This study is the first part of a deeper study on how to make networks train faster and be able to run on consumer grade GPUs in a competitive way (even if they are not SoTA). ..... TODO here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
