{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical Conllu file Exploration of  Universal Dependencies\n",
    "\n",
    "## Introduction\n",
    "\n",
    "While much work is being done in the current days on NLP and NLU, there is little work on describing why a certain length of transformer (or other as LSTM time steps) architecture has been chosen for the training, it is mostly arbitrary and depends on the goal of the work and resources available (mainly hardware). These decisions are hard once the model has been trained and there is nothing that can be done to extend the length of a transformer (for example) without having to retrain the entire network. There are however some works that tackle variable length sequences. \n",
    "\n",
    "This work presents the first complete analysis (from our knowledge) of the Universal Dependencies v2.6 dataset and presents the global and individual results by language.\n",
    "\n",
    "This work does not intend to be a conference level paper (that is why there are no references to papers on the subject), but an informational technical report that might help to better select the most effective compromise text or token length for your particular NLP application.\n",
    "\n",
    "The number of analyzed languages is 91 (out of 92), the token length is measured as the named UPOS tag in the dataset, while the character length is just that. There is no analysis on what constitutes a word or not, this means that a token includes the punctuation and other symbols presents in the text samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "[Universal Dependencies](https://universaldependencies.org/) [v2.6](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3226)\n",
    "\n",
    "The process is the same for any UD version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technology Stack\n",
    "\n",
    "The analysis was done with the following software stack:\n",
    "* Ubuntu 20.04\n",
    "* python 3.8 with all dependencies installed in a virtualenv \n",
    "* conllu files are dealt with [pyconll](https://github.com/pyconll/pyconll)\n",
    "* [pycountry  v19.8.18](https://pypi.org/project/pycountry/) is used to transform between country code and country names\n",
    "* [NumPy v1.18.4](https://numpy.org/) is used for numerical computation\n",
    "* [Pandas v1.0.4](https://pandas.pydata.org/) is used for DataFrame generation and making easier some operations\n",
    "* [SciPy v1.4.1](https://www.scipy.org/) is used for statistics\n",
    "* [Matplotlib v3.2.1](https://matplotlib.org/) was used for the first graphing iterations during the exploration stage\n",
    "* [Bokeh v2.0.2](https://docs.bokeh.org/en/latest/index.html) used for the latest graph and table generation to make interactive presentations\n",
    "* [Jupyter Lab v2.1.2](https://jupyterlab.readthedocs.io/en/stable/) was used during the entire process from starting the exploration to writing the current report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method \n",
    "\n",
    "1. A single zipped file containing the dataset was downloaded locally and uncompressed\n",
    "2. All the file tree is explored, a list of all the files is returned. All files, train, test and dev are used for this analysis to have the maximum amount of data available.\n",
    "3. undesired files are filtered out (used only during exploration to select different languages to explore and check errors)\n",
    "4.  each file:\n",
    " 1. Open the file\n",
    " 2. language code (2 or 3 letter) is extracted from the file name\n",
    " 3. sentence, form and upos fields are extracted for each file\n",
    "5. All the separate sentence, upos and forms data from each files are now merged in a single list for each type and enriched with language information\n",
    "6. Data is now set in a Pandas Dataframe to simplify handling (select by language in this case). For development or different needs a list of languages can be given to accelerate the processing (for example for exploration purposes)\n",
    "7. Every language is now processed independently and several possible distributions are tested, only the one with the best Kolmogorov-Smirnov test result is chosen as a result. This is the most time-expensive step in the pipe.\n",
    "8. Statistics are computed for every language\n",
    "9. Results are converted to json and saved in a single gz file (available to download in the github repository)\n",
    "10. Statistics tables and plots are done per language, graphs can be explored in the [project's github page](TODO)\n",
    "11. Tables are created for upos and text (character) length for all the languages together. These tables are available in the  [project's github page](TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The histograms show a skew on the distribution, this can be a skewed gaussian, a generalized gaussian or a beta distribution form. Due to this, I will be testing different distribution fits with the Kolmogorov-Smirnov test.\n",
    "\n",
    "There are many languages that do not have enough samples so the dsitribution fit will not be good  and errors will be big.\n",
    "This is not an issue  from the code point of view. The important thing is if this data is used, take into account the number of samples available.\n",
    "\n",
    "\n",
    "While doing this work I found quite interesting that are languages whose number of tokens or characters avoid certain bins in the histogram (Bulgarian, Breton Welsh, Danish, Slovak, Tamil and Thai are a few examples of this). This can mean that, either the language structure supports only those lengths, or that the analyzed dataset only contains samples that avoid some sentence lengths.\n",
    "\n",
    "For some languages the number of samples is too small to make any good assumption from the data.\n",
    "\n",
    "There is one missing language in the analysis, TODO HERE ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This work presents a sample length analysis by language on the UniversalDependencies v2.6 dataset presenting the statistics for all 91 of the 92 represented languages. The analysis then shows the length histograms by character and token length.\n",
    "\n",
    "The best compromise for choosing a sequence length on the NLP architecture for training will depend mostly on the requirements of the application, nevertheless with the numbers here you should be able to make an informed guess on what might be better for your case.\n",
    "\n",
    "We can see that having a multi-lingual approach will necessary make the needed sequences longer as there is a large variability on sequence length, but appliying to single language might allow you to optimize your neural architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "We are currently working on a more in depth analysis of the complete Gutenberg project dataset ( ~60K books in several languages) that will discriminate several other text characteristics.\n",
    "\n",
    "Stay tuned for those results ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors.ud_conllu_stats import *\n",
    "import preprocessors.ud_conllu_stats as udstats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:547: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/optimize/minpack.py:162: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:708: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  a/(b-1.0),\n",
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/stats/_continuous_distns.py:712: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  a*(a+1.0)/((b-2.0)*(b-1.0)),\n",
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1063: RuntimeWarning: invalid value encountered in subtract\n",
      "  mu2 = mu2p - mu * mu\n",
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:2407: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Lhat = muhat - Shat*mu\n",
      "/home/leo/venv3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py:1702: RuntimeWarning: divide by zero encountered in log\n",
      "  return log(self._pdf(x, *args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing lang qhe with Exception 'NoneType' object has no attribute 'name'\n",
      "processing af\n",
      "processing aii\n",
      "processing akk\n",
      "processing am\n",
      "processing ar\n",
      "processing be\n",
      "processing bg\n",
      "processing bho\n",
      "processing bm\n",
      "processing br\n",
      "processing bxr\n",
      "processing ca\n",
      "processing cop\n",
      "processing cs\n",
      "processing cu\n",
      "processing cy\n",
      "processing da\n",
      "processing de\n",
      "processing el\n",
      "processing en\n",
      "processing es\n",
      "processing et\n",
      "processing eu\n",
      "processing fa\n",
      "processing fi\n",
      "processing fo\n",
      "processing fr\n",
      "processing fro\n",
      "processing ga\n",
      "processing gd\n",
      "processing gl\n",
      "processing got\n",
      "processing grc\n",
      "processing gsw\n",
      "processing gun\n",
      "processing he\n",
      "processing hi\n",
      "processing hr\n",
      "processing hsb\n",
      "processing hu\n",
      "processing hy\n",
      "processing id\n",
      "processing is\n",
      "processing it\n",
      "processing ja\n",
      "processing kk\n",
      "processing kmr\n",
      "processing ko\n",
      "processing koi\n",
      "processing kpv\n",
      "processing krl\n",
      "processing la\n",
      "processing lt\n",
      "processing lv\n",
      "processing lzh\n",
      "processing mdf\n",
      "processing mr\n",
      "processing mt\n",
      "processing myv\n",
      "processing nl\n",
      "processing no\n",
      "processing olo\n",
      "processing orv\n",
      "processing pcm\n",
      "processing pl\n",
      "processing pt\n",
      "processing ro\n",
      "processing ru\n",
      "processing sa\n",
      "processing sk\n",
      "processing sl\n",
      "processing sme\n",
      "processing sms\n",
      "processing sq\n",
      "processing sr\n",
      "processing sv\n",
      "processing swl\n",
      "processing ta\n",
      "processing te\n",
      "processing th\n",
      "processing tl\n",
      "processing tr\n",
      "processing ug\n",
      "processing uk\n",
      "processing ur\n",
      "processing vi\n",
      "processing wbp\n",
      "processing wo\n",
      "processing yo\n",
      "processing yue\n",
      "processing zh\n",
      "Saving to conllu_stats.json.gz\n",
      "CPU times: user 14min 48s, sys: 3.6 s, total: 14min 52s\n",
      "Wall time: 16min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_stats = generate_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.32 s, sys: 8 ms, total: 6.33 s\n",
      "Wall time: 6.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_stats_copy = copy.deepcopy(all_stats)\n",
    "grids = udstats._generate_html_plots(all_stats_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.5 ms, sys: 0 ns, total: 49.5 ms\n",
      "Wall time: 48 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_stats_copy = copy.deepcopy(all_stats)\n",
    "upos_table, text_table = udstats._make_complete_stats_tables(all_stats_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 ms, sys: 0 ns, total: 29.8 ms\n",
      "Wall time: 28.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "upos_html = udstats._generate_html_table(upos_table, 'upos_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 27.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_html = udstats._generate_html_table(text_table, 'text_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
