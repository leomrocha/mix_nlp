{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A statistical Conllu file Exploration of  Universal Dependencies\n",
    "\n",
    "## Introduction\n",
    "\n",
    "While much work is being done in the current days on NLP and NLU, there is little work on describing why a certain length of transformer (or other as LSTM time steps) architecture has been chosen for the training, it is mostly arbitrary and depends on the goal of the work and resources available (mainly hardware). These decisions are hard once the model has been trained and there is nothing that can be done to extend the length of a transformer (for example) without having to retrain the entire network. There are however some works that tackle variable length sequences. \n",
    "\n",
    "This work presents a first complete analysis of the Universal Dependencies v2.6 dataset and presents the globan and individual results of each language present in the dataset.\n",
    "\n",
    "This work does not intend to be a conference level paper (that is why there are no references to all the papers on each subject), but an informational technical report that might help to better select the most effective compromise text or token length for your particular NLP application.\n",
    "\n",
    "The number of analyzed languages is 92, the token length is measured as the named UPOS tag in the dataset, while the character length is just that. There is no analysis on what constitutes a word or not, this means that a token includes the punctuiation and other symbols presents in the text samples. For ling√ºstic analysis purposes more de\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The histograms show a skew on the distribution, this can be a skewed gaussian, a generalized gaussian or a beta distribution form. Due to this, I will be testing different distribution fits with the Kolmogorov-Smirnov test.\n",
    "\n",
    "There are many languages that do not have enough samples so the dsitribution fit will not be good  and errors will be big.\n",
    "This is not an issue  from the code point of view. The important thing is if this data is used, take into account the number of samples available.\n",
    "\n",
    "\n",
    "While doing this work I found quite interesting that are languages whose number of tokens or characters avoid certain bins in the histogram (Bulgarian, Breton Welsh, Danish, Slovak, Tamil and Thai are a few examples of this). This can mean that, either the language structure supports only those lengths, or that the analyzed dataset only contains samples that avoid some sentence lengths.\n",
    "\n",
    "For some languages the number of samples is too small to make any good assumption from the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This work presents a sample length analysis by language on the UniversalDependencies v2.6 dataset presenting the statistics for all 92 represented languages. The analysis then shows the length histograms by character and token length.\n",
    "\n",
    "The best compromise for choosing a sequence length on the NLP architecture for training will depend mostly on the requirements of the applicatino, nevertheless with the numbers here you should be able to make an informed guess on what might be better for your case.\n",
    "\n",
    "We can see that having a multi-lingual approach will necessary make the needed sequences longer as there is a large variability on sequence length, but appliying to single language might allow you to optimize your neural architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "I am currently working on a more in depth analysis of the complete Gutenberg project dataset ( ~60K books in several languages) that will discriminate several other text characteristics.\n",
    "\n",
    "I also have started to work on a complete parsing of a few of the Wiktionary datasets.\n",
    "\n",
    "Stay tuned for those results ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors.ud_conllu_stats import *\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = conllu_process_get_2list(blacklist=blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "upos_data, deprel_data, sentences_data, forms_data = extract_data_from_fields(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# langs = ['es', 'fr', 'de', 'en']\n",
    "# langs_data = compute_distributions(upos_data, deprel_data, sentences_data, langs)\n",
    "langs_data = compute_distributions(upos_data, deprel_data, sentences_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well the stats file is quite big now, at 166MB.\n",
    "\n",
    "This is due to all the entire data functions (data, CDF, PDF) there, so an intermediate step would be to process this, plot the right graphs and then only save the graphs and then cut down the number of elements in the output.\n",
    "\n",
    "This should have taken care of much of the size issue, but for a website that will still be too much\n",
    "\n",
    "Also there are some other approaches and the idea is to think what the user would be looking for when reading the reports so:\n",
    "Reading by language: each language can have it's own file, this means that 166MB/92 ~< 2MB per file. \n",
    "\n",
    "\n",
    "\n",
    "Also have a file with the table of the stats only, no need to have the graphs there. In this way there is an easy comparison.\n",
    "The statistics should be computed and displayed for upos, deprel and text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = generate_files(blacklist=[], saveto='conllu_stats.json.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "upos_table, deprel_table, text_table = stats_dict2table(all_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tables = [upos_table, deprel_table, text_table]\n",
    "\n",
    "intervals = ['intervals_99', 'intervals_98', 'intervals_95', 'intervals_90', 'intervals_85', 'intervals_80']\n",
    "for df in df_tables:\n",
    "    for interval in intervals:\n",
    "        df[[interval+'_low', interval+'_high']] = pd.DataFrame(df[interval].tolist(), index=df.index)\n",
    "    df.drop(columns=intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bokeh.models.widgets import DataTable, DateFormatter, TableColumn\n",
    "from bokeh.models import ColumnDataSource, DataTable, DateFormatter, TableColumn\n",
    "\n",
    "df_tables = [upos_table, deprel_table, text_table]\n",
    "bk_tables = []\n",
    "\n",
    "for table in df_tables:\n",
    "    Columns = [TableColumn(field=Ci, title=Ci) for Ci in table.columns] # bokeh columns\n",
    "    data_table = DataTable(columns=Columns, source=ColumnDataSource(table)) # bokeh table\n",
    "    bk_tables.append(data_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upos_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upos_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(bk_tables[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frstats = all_stats['fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frstats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "# from bokeh.sampledata.stocks import AAPL, GOOG, IBM, MSFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf, pdf = frstats['text_functions']['cdf'], frstats['text_functions']['pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf100 = resample(cdf, 100)\n",
    "pdf100 = resample(pdf, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(frstats['text_len'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = resample(cdf, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = resample(pdf, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import LinearAxis, Range1d, HoverTool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(frstats['text_len'])\n",
    "x = np.linspace(0, max_len, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = plot.circle(x, y, size=10,\n",
    "                fill_color=\"grey\", hover_fill_color=\"firebrick\",\n",
    "                fill_alpha=0.05, hover_alpha=0.3,\n",
    "                line_color=None, hover_line_color=\"white\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make it even better being able to change the Sizing mode from a dropdown menu ?\n",
    "\n",
    "hover = HoverTool(\n",
    "#     names=[\"hist\"],\n",
    "    tooltips=[\n",
    "#         (\"index\", \"$index\"),\n",
    "        (\"Count\", \"@hist\"),\n",
    "        (\"pdf\", \"@pdf\"),\n",
    "        (\"cdf\", \"@cdf\"),\n",
    "    ],\n",
    "\n",
    "#     formatters={\n",
    "#         '@date'        : 'datetime', # use 'datetime' formatter for '@date' field\n",
    "#         '@{adj close}' : 'printf',   # use 'printf' formatter for '@{adj close}' field\n",
    "#         '@numeral': '(.00)'                             # use default 'numeral' formatter for other fields\n",
    "#     },\n",
    "#     renderers=[cr],\n",
    "    # display a tooltip whenever the cursor is vertically in line with a glyph\n",
    "    mode='vline',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(title, data_source):\n",
    "\n",
    "    p = figure(title=title, background_fill_color=\"#fafafa\", \n",
    "               plot_height=500,  sizing_mode=\"stretch_width\",\n",
    "               tools=\"crosshair,pan,wheel_zoom,box_zoom,zoom_in,zoom_out,undo,redo,reset\",\n",
    "                toolbar_location=\"left\",\n",
    "               output_backend=\"webgl\")\n",
    "    p.add_tools(hover)\n",
    "    # p = figure(title=title, tools='', background_fill_color=\"#fafafa\")\n",
    "    p.xaxis.axis_label = 'Length'\n",
    "    p.yaxis.axis_label = 'Count'\n",
    "    # second axe, probability\n",
    "    p.extra_y_ranges = {\"Pr(x)\": Range1d(start=0., end=1.)}\n",
    "    p.add_layout(LinearAxis(y_range_name=\"Pr(x)\", axis_label='Pr(x)'), 'right')\n",
    "    p.quad(name='hist', top='hist', bottom=0, left='bin_edges_left', right='bin_edges_right',\n",
    "           fill_color=\"blue\", line_color=\"white\", alpha=0.5, legend_label=\"Freq.\", source=data_source)\n",
    "    p.line(name='PDF', x='x', y='pdf', line_color=\"green\", line_width=4, alpha=0.7, legend_label=\"PDF\", y_range_name=\"Pr(x)\", source=data_source)\n",
    "    p.line(name='CDF', x='x', y='cdf', line_color=\"red\", line_width=2, alpha=0.7, legend_label=\"CDF\", y_range_name=\"Pr(x)\", source=data_source)\n",
    "\n",
    "\n",
    "    p.y_range.start = 0\n",
    "\n",
    "    p.title.align='center'\n",
    "    p.legend.location = \"center_right\"\n",
    "    #     p.legend.location = \"bottom_right\"\n",
    "    p.legend.background_fill_color = \"#fefefe\"\n",
    "    p.grid.grid_line_color=\"grey\"\n",
    "    #     p.legend.click_policy=\"mute\"\n",
    "    p.legend.click_policy=\"hide\"\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ColumnDataSource({'hist':hist,\n",
    "                                'bin_edges_left': bin_edges[:-1],\n",
    "                                'bin_edges_right': bin_edges[1:],\n",
    "                                'x': x,\n",
    "                                'pdf': pdf100,\n",
    "                                'cdf': cdf100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_txt = make_plot('frech, txt-len', data_source)\n",
    "show(p_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normal Distribution\n",
    "\n",
    "# bins = 100\n",
    "\n",
    "# mu, sigma = 0, 0.5\n",
    "\n",
    "# measured = np.random.normal(mu, sigma, 1000)\n",
    "# hist, edges = np.histogram(measured, density=True, bins=50)\n",
    "\n",
    "# x = np.linspace(-2, 2, 1000)\n",
    "# pdf = 1/(sigma * np.sqrt(2*np.pi)) * np.exp(-(x-mu)**2 / (2*sigma**2))\n",
    "# cdf = (1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2\n",
    "\n",
    "# p1 = make_plot(\"Normal Distribution (Œº=0, œÉ=0.5)\", hist, edges, x, pdf, cdf)\n",
    "\n",
    "# # Log-Normal Distribution\n",
    "\n",
    "# mu, sigma = 0, 0.5\n",
    "\n",
    "# measured = np.random.lognormal(mu, sigma, 1000)\n",
    "# hist, edges = np.histogram(measured, density=True, bins=50)\n",
    "\n",
    "# x = np.linspace(0.0001, 8.0, 1000)\n",
    "# pdf = 1/(x* sigma * np.sqrt(2*np.pi)) * np.exp(-(np.log(x)-mu)**2 / (2*sigma**2))\n",
    "# cdf = (1+scipy.special.erf((np.log(x)-mu)/(np.sqrt(2)*sigma)))/2\n",
    "\n",
    "# p2 = make_plot(\"Log Normal Distribution (Œº=0, œÉ=0.5)\", hist, edges, x, pdf, cdf)\n",
    "\n",
    "# # Gamma Distribution\n",
    "\n",
    "# k, theta = 7.5, 1.0\n",
    "\n",
    "# measured = np.random.gamma(k, theta, 1000)\n",
    "# hist, edges = np.histogram(measured, density=True, bins=50)\n",
    "\n",
    "# x = np.linspace(0.0001, 20.0, 1000)\n",
    "# pdf = x**(k-1) * np.exp(-x/theta) / (theta**k * scipy.special.gamma(k))\n",
    "# cdf = scipy.special.gammainc(k, x/theta)\n",
    "\n",
    "# p3 = make_plot(\"Gamma Distribution (k=7.5, Œ∏=1)\", hist, edges, x, pdf, cdf)\n",
    "\n",
    "# # Weibull Distribution\n",
    "\n",
    "# lam, k = 1, 1.25\n",
    "# measured = lam*(-np.log(np.random.uniform(0, 1, 1000)))**(1/k)\n",
    "# hist, edges = np.histogram(measured, density=True, bins=50)\n",
    "\n",
    "# x = np.linspace(0.0001, 8, 1000)\n",
    "# pdf = (k/lam)*(x/lam)**(k-1) * np.exp(-(x/lam)**k)\n",
    "# cdf = 1 - np.exp(-(x/lam)**k)\n",
    "\n",
    "# p4 = make_plot(\"Weibull Distribution (Œª=1, k=1.25)\", hist, edges, x, pdf, cdf)\n",
    "\n",
    "# # output_file('histogram.html', title=\"histogram.py example\")\n",
    "\n",
    "# show(gridplot([p1,p2,p3,p4], ncols=2, plot_width=400, plot_height=400, toolbar_location=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# res_2 = conllu_process_get_2list(blacklist=blacklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# upos_data_2, sentences_data_2, forms_data_2 = extract_data_from_fields(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# langs_2 = ['es', 'fr', 'de', 'en']\n",
    "# langs_data_2 = compute_distributions(upos_data_2, sentences_data_2, langs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_stats(distrib, distrib_params, data, n_bins=100, n_samples=100):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     :param distrib: distribution function (scipy.stats.[beta|norm|....]) \n",
    "#     :param distrib_params: parameters of the distribution\n",
    "#     :param data:\n",
    "#     :param n_bins: number of bins to compute for the histograms.\n",
    "#     :param n_samples: number of samples for the CDF and PDF functions\n",
    "#     :return: (stats, {cdf,pdf})\n",
    "#     \"\"\"\n",
    "#     try:  # if data is a pandas dataframe (which it is) TODO cleanup these dirty things\n",
    "#         data = data.to_numpy()\n",
    "#     except:\n",
    "#         pass\n",
    "#     mskv = [None, None, None, None]\n",
    "#     t_mskv = distrib.stats(*distrib_params)\n",
    "#     for i in range(len(t_mskv)):  # mean, variance, skew, kurtosis -> variable length\n",
    "#         mskv[i] = t_mskv[i]\n",
    "#     ret_stats = {\n",
    "#         'mean': mskv[0],  # mean, variance, skew, kurtosis -> variable length\n",
    "#         'variance': mskv[1],\n",
    "#         'skew': mskv[2],\n",
    "#         'kurtosis': mskv[3],\n",
    "#         'median': distrib.median(*distrib_params),\n",
    "#         'std': distrib.std(*distrib_params),\n",
    "#         'intervals': {'99': distrib.interval(0.99, *distrib_params),\n",
    "#                       '98': distrib.interval(0.98, *distrib_params),\n",
    "#                       '95': distrib.interval(0.95, *distrib_params),\n",
    "#                       '90': distrib.interval(0.90, *distrib_params),\n",
    "#                       '85': distrib.interval(0.85, *distrib_params),\n",
    "#                       '80': distrib.interval(0.8, *distrib_params),\n",
    "#                       }\n",
    "#     }\n",
    "#     max_len = max(data)\n",
    "#     x = np.linspace(0, max_len, 100)\n",
    "#     hist, bin_edges = np.histogram(data, bins=n_bins)  # (hist, bin_edges)\n",
    "#     # the function computation is to make life easy when drawing with bokeh ... some points still to clarify\n",
    "#     # for this n_samples needs to be the same as n_bins\n",
    "#     ret_foo = {'x': x,\n",
    "#                'hist': hist,\n",
    "#                'bin_edges': bin_edges,\n",
    "#                # 'bin_edges_left': bin_edges[:-1],\n",
    "#                # 'bin_edges_right': bin_edges[1:],\n",
    "#                'cdf': resample(distrib.cdf(x, *distrib_params), n_samples),\n",
    "#                'pdf': resample(distrib.pdf(x, *distrib_params), n_samples)\n",
    "#                }\n",
    "#     return ret_stats, ret_foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_lang_stats(lang_data, distributions=DISTRIBUTIONS):\n",
    "#     upos_distrib = distributions[lang_data['upos_distrib'][0]]\n",
    "#     upos_distrib_params = lang_data['upos_distrib'][2]\n",
    "#     #     print('upos', upos_distrib, upos_distrib_params)\n",
    "#     upos_data = lang_data['upos_len']\n",
    "#     upos_stats, upos_functions = _get_stats(upos_distrib, upos_distrib_params, upos_data)\n",
    "#     #\n",
    "#     # deprel_distrib = distributions[lang_data['deprel_distrib'][0]]\n",
    "#     # deprel_distrib_params = lang_data['deprel_distrib'][2]\n",
    "#     # #     print('deprel', deprel_distrib, deprel_distrib_params)\n",
    "#     # deprel_data = lang_data['deprel_len']\n",
    "#     # deprel_stats, deprel_functions = _get_stats(deprel_distrib, deprel_distrib_params, deprel_data)\n",
    "#     #\n",
    "#     text_distrib = distributions[lang_data['text_distrib'][0]]\n",
    "#     text_distrib_params = lang_data['text_distrib'][2]\n",
    "#     #     print('text', text_distrib, text_distrib_params)\n",
    "#     text_data = lang_data['text_len']\n",
    "#     text_stats, text_functions = _get_stats(text_distrib, text_distrib_params, text_data)\n",
    "\n",
    "#     lang_data['upos_stats'] = upos_stats\n",
    "#     # lang_data['deprel_stats'] = deprel_stats\n",
    "#     lang_data['text_stats'] = text_stats\n",
    "\n",
    "#     lang_data['upos_functions'] = upos_functions\n",
    "#     # lang_data['deprel_functions'] = deprel_functions\n",
    "#     lang_data['text_functions'] = text_functions\n",
    "\n",
    "#     return lang_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_stats = {}\n",
    "\n",
    "# for lang, lang_data in langs_data_2.items():\n",
    "#     print('processing {}'.format(lang))\n",
    "#     all_stats[lang] = _get_lang_stats(lang_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions=DISTRIBUTIONS\n",
    "\n",
    "# lang_data = langs_data_2['fr']\n",
    "# text_distrib = distributions[lang_data['text_distrib'][0]]\n",
    "# text_distrib_params = lang_data['text_distrib'][2]\n",
    "# #     print('text', text_distrib, text_distrib_params)\n",
    "# text_data = lang_data['text_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = text_data\n",
    "# n_bins = 100\n",
    "# max_len = max(data)\n",
    "# x = np.linspace(0, max_len, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist, bin_edges = np.histogram(data, bins=n_bins)  # (hist, bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_stats, text_functions = _get_stats(text_distrib, text_distrib_params, text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(text_functions['cdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_name, dist_p, (alpha, beta, loc, scale) = lang_data['text_distrib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.beta.cdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def _get_data_source(lang_data):\n",
    "\n",
    "    \n",
    "#     data_source = ColumnDataSource({'hist':hist,\n",
    "#                                     'bin_edges_left': bin_edges[:-1],\n",
    "#                                     'bin_edges_right': bin_edges[1:],\n",
    "#                                     'x': x,\n",
    "#                                     'pdf': pdf100,\n",
    "#                                     'cdf': cdf100})\n",
    "    \n",
    "#     return title, data_source_upos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title, data_source = _get_data_source()\n",
    "# p_txt = make_plot('frech, txt-len', data_source)\n",
    "# show(p_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frstats = langs_data_2['fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# frstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf, pdf = frstats['text_functions']['cdf'], frstats['text_functions']['pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
